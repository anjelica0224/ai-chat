{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "085a6ad7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T18:26:57.129916Z",
     "iopub.status.busy": "2025-11-27T18:26:57.129637Z",
     "iopub.status.idle": "2025-11-27T20:32:45.547578Z",
     "shell.execute_reply": "2025-11-27T20:32:45.546347Z"
    },
    "papermill": {
     "duration": 7548.423194,
     "end_time": "2025-11-27T20:32:45.549192",
     "exception": false,
     "start_time": "2025-11-27T18:26:57.125998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE = 41 (BLANK + 39 phonemes + silence)\n",
      "Phoneme set: ['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY', 'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH', 'UW', 'V', 'W', 'Y', 'Z', 'ZH', ' | ']\n",
      "============================================================\n",
      "LOADING FULL DATASET...\n",
      "============================================================\n",
      "\n",
      "Found 45 training files\n",
      "Found 41 validation files\n",
      "\n",
      "Loading training trials...\n",
      "  data_train.hdf5: 288 trials\n",
      "  data_train.hdf5: 348 trials\n",
      "  data_train.hdf5: 197 trials\n",
      "  data_train.hdf5: 278 trials\n",
      "  data_train.hdf5: 88 trials\n",
      "  data_train.hdf5: 150 trials\n",
      "  data_train.hdf5: 297 trials\n",
      "  data_train.hdf5: 322 trials\n",
      "  data_train.hdf5: 245 trials\n",
      "  data_train.hdf5: 153 trials\n",
      "  data_train.hdf5: 218 trials\n",
      "  data_train.hdf5: 174 trials\n",
      "  data_train.hdf5: 284 trials\n",
      "  data_train.hdf5: 155 trials\n",
      "  data_train.hdf5: 239 trials\n",
      "  data_train.hdf5: 98 trials\n",
      "  data_train.hdf5: 134 trials\n",
      "  data_train.hdf5: 149 trials\n",
      "  data_train.hdf5: 80 trials\n",
      "  data_train.hdf5: 100 trials\n",
      "  data_train.hdf5: 60 trials\n",
      "  data_train.hdf5: 198 trials\n",
      "  data_train.hdf5: 228 trials\n",
      "  data_train.hdf5: 198 trials\n",
      "  data_train.hdf5: 131 trials\n",
      "  data_train.hdf5: 135 trials\n",
      "  data_train.hdf5: 198 trials\n",
      "  data_train.hdf5: 193 trials\n",
      "  data_train.hdf5: 219 trials\n",
      "  data_train.hdf5: 163 trials\n",
      "  data_train.hdf5: 239 trials\n",
      "  data_train.hdf5: 246 trials\n",
      "  data_train.hdf5: 364 trials\n",
      "  data_train.hdf5: 150 trials\n",
      "  data_train.hdf5: 110 trials\n",
      "  data_train.hdf5: 90 trials\n",
      "  data_train.hdf5: 169 trials\n",
      "  data_train.hdf5: 160 trials\n",
      "  data_train.hdf5: 161 trials\n",
      "  data_train.hdf5: 106 trials\n",
      "  data_train.hdf5: 163 trials\n",
      "  data_train.hdf5: 59 trials\n",
      "  data_train.hdf5: 101 trials\n",
      "  data_train.hdf5: 165 trials\n",
      "  data_train.hdf5: 69 trials\n",
      "\n",
      "Total trials found: 8072\n",
      "\n",
      "Loading validation trials...\n",
      "  data_val.hdf5: 35 trials\n",
      "  data_val.hdf5: 49 trials\n",
      "  data_val.hdf5: 48 trials\n",
      "  data_val.hdf5: 25 trials\n",
      "  data_val.hdf5: 25 trials\n",
      "  data_val.hdf5: 49 trials\n",
      "  data_val.hdf5: 34 trials\n",
      "  data_val.hdf5: 35 trials\n",
      "  data_val.hdf5: 48 trials\n",
      "  data_val.hdf5: 44 trials\n",
      "  data_val.hdf5: 36 trials\n",
      "  data_val.hdf5: 17 trials\n",
      "  data_val.hdf5: 44 trials\n",
      "  data_val.hdf5: 44 trials\n",
      "  data_val.hdf5: 9 trials\n",
      "  data_val.hdf5: 33 trials\n",
      "  data_val.hdf5: 50 trials\n",
      "  data_val.hdf5: 15 trials\n",
      "  data_val.hdf5: 25 trials\n",
      "  data_val.hdf5: 20 trials\n",
      "  data_val.hdf5: 44 trials\n",
      "  data_val.hdf5: 34 trials\n",
      "  data_val.hdf5: 50 trials\n",
      "  data_val.hdf5: 25 trials\n",
      "  data_val.hdf5: 30 trials\n",
      "  data_val.hdf5: 50 trials\n",
      "  data_val.hdf5: 23 trials\n",
      "  data_val.hdf5: 24 trials\n",
      "  data_val.hdf5: 48 trials\n",
      "  data_val.hdf5: 48 trials\n",
      "  data_val.hdf5: 25 trials\n",
      "  data_val.hdf5: 25 trials\n",
      "  data_val.hdf5: 48 trials\n",
      "  data_val.hdf5: 46 trials\n",
      "  data_val.hdf5: 48 trials\n",
      "  data_val.hdf5: 23 trials\n",
      "  data_val.hdf5: 47 trials\n",
      "  data_val.hdf5: 24 trials\n",
      "  data_val.hdf5: 24 trials\n",
      "  data_val.hdf5: 30 trials\n",
      "  data_val.hdf5: 25 trials\n",
      "\n",
      "Total trials found: 1426\n",
      "Dataset initialized with 8072 trials\n",
      "Dataset initialized with 1426 trials\n",
      "\n",
      "DataLoader created:\n",
      "  Train batches: 2018\n",
      "  Val batches: 713\n",
      "\n",
      "Model parameters: 3,301,417\n",
      "\n",
      "============================================================\n",
      "STARTING TRAINING ON FULL DATASET\n",
      "============================================================\n",
      "  Batch 100/2018 | Loss: 3.3741\n",
      "  Batch 200/2018 | Loss: 3.3897\n",
      "  Batch 300/2018 | Loss: 3.3455\n",
      "  Batch 400/2018 | Loss: 3.2666\n",
      "  Batch 500/2018 | Loss: 3.2384\n",
      "  Batch 600/2018 | Loss: 3.2258\n",
      "  Batch 700/2018 | Loss: 3.2590\n",
      "  Batch 800/2018 | Loss: 3.1918\n",
      "  Batch 900/2018 | Loss: 3.1164\n",
      "  Batch 1000/2018 | Loss: 3.2290\n",
      "  Batch 1100/2018 | Loss: 3.1928\n",
      "  Batch 1200/2018 | Loss: 2.9604\n",
      "  Batch 1300/2018 | Loss: 3.0053\n",
      "  Batch 1400/2018 | Loss: 2.9233\n",
      "  Batch 1500/2018 | Loss: 2.8918\n",
      "  Batch 1600/2018 | Loss: 2.7554\n",
      "  Batch 1700/2018 | Loss: 2.8183\n",
      "  Batch 1800/2018 | Loss: 2.7905\n",
      "  Batch 1900/2018 | Loss: 2.7362\n",
      "  Batch 2000/2018 | Loss: 2.5454\n",
      "\n",
      "EPOCH 1/30 | Avg Loss: 4.1187\n",
      "  Batch 100/2018 | Loss: 2.8108\n",
      "  Batch 200/2018 | Loss: 2.7108\n",
      "  Batch 300/2018 | Loss: 2.3678\n",
      "  Batch 400/2018 | Loss: 2.5341\n",
      "  Batch 500/2018 | Loss: 2.4360\n",
      "  Batch 600/2018 | Loss: 2.4135\n",
      "  Batch 700/2018 | Loss: 2.6626\n",
      "  Batch 800/2018 | Loss: 2.5015\n",
      "  Batch 900/2018 | Loss: 2.5667\n",
      "  Batch 1000/2018 | Loss: 2.4197\n",
      "  Batch 1100/2018 | Loss: 2.4696\n",
      "  Batch 1200/2018 | Loss: 2.0223\n",
      "  Batch 1300/2018 | Loss: 2.1094\n",
      "  Batch 1400/2018 | Loss: 2.3434\n",
      "  Batch 1500/2018 | Loss: 2.6862\n",
      "  Batch 1600/2018 | Loss: 2.2950\n",
      "  Batch 1700/2018 | Loss: 2.4605\n",
      "  Batch 1800/2018 | Loss: 2.1602\n",
      "  Batch 1900/2018 | Loss: 2.4846\n",
      "  Batch 2000/2018 | Loss: 2.4877\n",
      "\n",
      "EPOCH 2/30 | Avg Loss: 2.4375\n",
      "  Batch 100/2018 | Loss: 1.9320\n",
      "  Batch 200/2018 | Loss: 2.1185\n",
      "  Batch 300/2018 | Loss: 2.1220\n",
      "  Batch 400/2018 | Loss: 2.1983\n",
      "  Batch 500/2018 | Loss: 1.9118\n",
      "  Batch 600/2018 | Loss: 1.8913\n",
      "  Batch 700/2018 | Loss: 2.5442\n",
      "  Batch 800/2018 | Loss: 1.6755\n",
      "  Batch 900/2018 | Loss: 1.9488\n",
      "  Batch 1000/2018 | Loss: 2.5808\n",
      "  Batch 1100/2018 | Loss: 1.6891\n",
      "  Batch 1200/2018 | Loss: 2.2126\n",
      "  Batch 1300/2018 | Loss: 1.7926\n",
      "  Batch 1400/2018 | Loss: 1.9477\n",
      "  Batch 1500/2018 | Loss: 1.8869\n",
      "  Batch 1600/2018 | Loss: 1.8963\n",
      "  Batch 1700/2018 | Loss: 2.4012\n",
      "  Batch 1800/2018 | Loss: 2.6405\n",
      "  Batch 1900/2018 | Loss: 1.9402\n",
      "  Batch 2000/2018 | Loss: 1.9486\n",
      "\n",
      "EPOCH 3/30 | Avg Loss: 2.0133\n",
      "\n",
      "Running full validation...\n",
      "Starting validation...\n",
      "  Validated 50/713 batches\n",
      "  Validated 100/713 batches\n",
      "  Validated 150/713 batches\n",
      "  Validated 200/713 batches\n",
      "  Validated 250/713 batches\n",
      "  Validated 300/713 batches\n",
      "  Validated 350/713 batches\n",
      "  Validated 400/713 batches\n",
      "  Validated 450/713 batches\n",
      "  Validated 500/713 batches\n",
      "  Validated 550/713 batches\n",
      "  Validated 600/713 batches\n",
      "  Validated 650/713 batches\n",
      "  Validated 700/713 batches\n",
      "\n",
      "Validation Results:\n",
      "  Total edit distance: 32692\n",
      "  Total sequence length: 41392\n",
      "  PER: 78.98%\n",
      "✓ NEW BEST MODEL SAVED!\n",
      "\n",
      "Sample predictions (phonemes):\n",
      "Sample 1:\n",
      "  Truth: Y UW  |  K AE N  |  S IY  |  DH AH  |  K OW D  |  AE T  |  DH IH S  |  P OY N T  |  AE Z  |  W EH L  | \n",
      "  Pred:   |   |   |   |   |   |   |   |   | \n",
      "Sample 2:\n",
      "  Truth: HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AA S T  |  D AW N  | \n",
      "  Pred:   |   |   |   |   |   | \n",
      "Sample 3:\n",
      "  Truth: N AA T  |  T UW  |  K AA N T R AH V ER SH AH L  | \n",
      "  Pred:   |   |   |   |   |   |   |   | \n",
      "------------------------------------------------------------\n",
      "  Batch 100/2018 | Loss: 1.6813\n",
      "  Batch 200/2018 | Loss: 2.0828\n",
      "  Batch 300/2018 | Loss: 1.7457\n",
      "  Batch 400/2018 | Loss: 1.5325\n",
      "  Batch 500/2018 | Loss: 1.4438\n",
      "  Batch 600/2018 | Loss: 1.9276\n",
      "  Batch 700/2018 | Loss: 1.6497\n",
      "  Batch 800/2018 | Loss: 1.7546\n",
      "  Batch 900/2018 | Loss: 1.5865\n",
      "  Batch 1000/2018 | Loss: 1.5513\n",
      "  Batch 1100/2018 | Loss: 1.4427\n",
      "  Batch 1200/2018 | Loss: 1.4826\n",
      "  Batch 1300/2018 | Loss: 2.0815\n",
      "  Batch 1400/2018 | Loss: 2.1991\n",
      "  Batch 1500/2018 | Loss: 2.0777\n",
      "  Batch 1600/2018 | Loss: 1.3264\n",
      "  Batch 1700/2018 | Loss: 1.6324\n",
      "  Batch 1800/2018 | Loss: 1.4857\n",
      "  Batch 1900/2018 | Loss: 1.6286\n",
      "  Batch 2000/2018 | Loss: 1.2859\n",
      "\n",
      "EPOCH 4/30 | Avg Loss: 1.7168\n",
      "  Batch 100/2018 | Loss: 1.7838\n",
      "  Batch 200/2018 | Loss: 1.2000\n",
      "  Batch 300/2018 | Loss: 1.8731\n",
      "  Batch 400/2018 | Loss: 1.8230\n",
      "  Batch 500/2018 | Loss: 1.4922\n",
      "  Batch 600/2018 | Loss: 1.4061\n",
      "  Batch 700/2018 | Loss: 1.4796\n",
      "  Batch 800/2018 | Loss: 1.5740\n",
      "  Batch 900/2018 | Loss: 1.3907\n",
      "  Batch 1000/2018 | Loss: 2.0304\n",
      "  Batch 1100/2018 | Loss: 1.6114\n",
      "  Batch 1200/2018 | Loss: 1.7205\n",
      "  Batch 1300/2018 | Loss: 1.4232\n",
      "  Batch 1400/2018 | Loss: 1.6711\n",
      "  Batch 1500/2018 | Loss: 1.2277\n",
      "  Batch 1600/2018 | Loss: 1.6265\n",
      "  Batch 1700/2018 | Loss: 1.3796\n",
      "  Batch 1800/2018 | Loss: 1.9247\n",
      "  Batch 1900/2018 | Loss: 1.4920\n",
      "  Batch 2000/2018 | Loss: 1.5810\n",
      "\n",
      "EPOCH 5/30 | Avg Loss: 1.5179\n",
      "  Batch 100/2018 | Loss: 1.3685\n",
      "  Batch 200/2018 | Loss: 1.1798\n",
      "  Batch 300/2018 | Loss: 1.8877\n",
      "  Batch 400/2018 | Loss: 1.3083\n",
      "  Batch 500/2018 | Loss: 1.3349\n",
      "  Batch 600/2018 | Loss: 1.6122\n",
      "  Batch 700/2018 | Loss: 1.7236\n",
      "  Batch 800/2018 | Loss: 1.4072\n",
      "  Batch 900/2018 | Loss: 1.6974\n",
      "  Batch 1000/2018 | Loss: 1.2149\n",
      "  Batch 1100/2018 | Loss: 1.0707\n",
      "  Batch 1200/2018 | Loss: 1.1612\n",
      "  Batch 1300/2018 | Loss: 1.1822\n",
      "  Batch 1400/2018 | Loss: 1.1921\n",
      "  Batch 1500/2018 | Loss: 1.0319\n",
      "  Batch 1600/2018 | Loss: 1.3790\n",
      "  Batch 1700/2018 | Loss: 0.9713\n",
      "  Batch 1800/2018 | Loss: 1.4874\n",
      "  Batch 1900/2018 | Loss: 0.7258\n",
      "  Batch 2000/2018 | Loss: 1.1169\n",
      "\n",
      "EPOCH 6/30 | Avg Loss: 1.3114\n",
      "\n",
      "Running full validation...\n",
      "Starting validation...\n",
      "  Validated 50/713 batches\n",
      "  Validated 100/713 batches\n",
      "  Validated 150/713 batches\n",
      "  Validated 200/713 batches\n",
      "  Validated 250/713 batches\n",
      "  Validated 300/713 batches\n",
      "  Validated 350/713 batches\n",
      "  Validated 400/713 batches\n",
      "  Validated 450/713 batches\n",
      "  Validated 500/713 batches\n",
      "  Validated 550/713 batches\n",
      "  Validated 600/713 batches\n",
      "  Validated 650/713 batches\n",
      "  Validated 700/713 batches\n",
      "\n",
      "Validation Results:\n",
      "  Total edit distance: 21813\n",
      "  Total sequence length: 41392\n",
      "  PER: 52.70%\n",
      "✓ NEW BEST MODEL SAVED!\n",
      "\n",
      "Sample predictions (phonemes):\n",
      "Sample 1:\n",
      "  Truth: Y UW  |  K AE N  |  S IY  |  DH AH  |  K OW D  |  AE T  |  DH IH S  |  P OY N T  |  AE Z  |  W EH L  | \n",
      "  Pred:  Y UW  |  K D  |  IY  |  DH AH  |  K D  |  IH P T  |  DH IH  |  P OY  |  Z W  | \n",
      "Sample 2:\n",
      "  Truth: HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AA S T  |  D AW N  | \n",
      "  Pred:  HH AW AW  |  D AH S  |  IH T  |  P  |  DH DH AH  |  AH S  |  D  | \n",
      "Sample 3:\n",
      "  Truth: N AA T  |  T UW  |  K AA N T R AH V ER SH AH L  | \n",
      "  Pred:  N D  |  UW  |  AH L  |   | \n",
      "------------------------------------------------------------\n",
      "  Batch 100/2018 | Loss: 1.2298\n",
      "  Batch 200/2018 | Loss: 0.6370\n",
      "  Batch 300/2018 | Loss: 1.0875\n",
      "  Batch 400/2018 | Loss: 0.6797\n",
      "  Batch 500/2018 | Loss: 0.9449\n",
      "  Batch 600/2018 | Loss: 1.5172\n",
      "  Batch 700/2018 | Loss: 1.3393\n",
      "  Batch 800/2018 | Loss: 1.0750\n",
      "  Batch 900/2018 | Loss: 0.9665\n",
      "  Batch 1000/2018 | Loss: 1.1681\n",
      "  Batch 1100/2018 | Loss: 0.8106\n",
      "  Batch 1200/2018 | Loss: 0.7425\n",
      "  Batch 1300/2018 | Loss: 1.3110\n",
      "  Batch 1400/2018 | Loss: 0.8510\n",
      "  Batch 1500/2018 | Loss: 0.8145\n",
      "  Batch 1600/2018 | Loss: 0.8531\n",
      "  Batch 1700/2018 | Loss: 1.5060\n",
      "  Batch 1800/2018 | Loss: 0.7669\n",
      "  Batch 1900/2018 | Loss: 1.4026\n",
      "  Batch 2000/2018 | Loss: 1.0017\n",
      "\n",
      "EPOCH 7/30 | Avg Loss: 1.1145\n",
      "  Batch 100/2018 | Loss: 0.9431\n",
      "  Batch 200/2018 | Loss: 0.9473\n",
      "  Batch 300/2018 | Loss: 1.0101\n",
      "  Batch 400/2018 | Loss: 1.0080\n",
      "  Batch 500/2018 | Loss: 0.8816\n",
      "  Batch 600/2018 | Loss: 1.3564\n",
      "  Batch 700/2018 | Loss: 1.0367\n",
      "  Batch 800/2018 | Loss: 0.4517\n",
      "  Batch 900/2018 | Loss: 0.5744\n",
      "  Batch 1000/2018 | Loss: 1.0062\n",
      "  Batch 1100/2018 | Loss: 1.2214\n",
      "  Batch 1200/2018 | Loss: 1.1619\n",
      "  Batch 1300/2018 | Loss: 1.2895\n",
      "  Batch 1400/2018 | Loss: 0.7966\n",
      "  Batch 1500/2018 | Loss: 1.2844\n",
      "  Batch 1600/2018 | Loss: 0.8895\n",
      "  Batch 1700/2018 | Loss: 0.8284\n",
      "  Batch 1800/2018 | Loss: 0.5492\n",
      "  Batch 1900/2018 | Loss: 0.5478\n",
      "  Batch 2000/2018 | Loss: 0.7311\n",
      "\n",
      "EPOCH 8/30 | Avg Loss: 0.9760\n",
      "  Batch 100/2018 | Loss: 1.0541\n",
      "  Batch 200/2018 | Loss: 1.1695\n",
      "  Batch 300/2018 | Loss: 0.6449\n",
      "  Batch 400/2018 | Loss: 0.8909\n",
      "  Batch 500/2018 | Loss: 1.0404\n",
      "  Batch 600/2018 | Loss: 0.7054\n",
      "  Batch 700/2018 | Loss: 0.3122\n",
      "  Batch 800/2018 | Loss: 0.5468\n",
      "  Batch 900/2018 | Loss: 0.8345\n",
      "  Batch 1000/2018 | Loss: 0.9854\n",
      "  Batch 1100/2018 | Loss: 0.5250\n",
      "  Batch 1200/2018 | Loss: 0.9322\n",
      "  Batch 1300/2018 | Loss: 0.5758\n",
      "  Batch 1400/2018 | Loss: 0.6784\n",
      "  Batch 1500/2018 | Loss: 0.7523\n",
      "  Batch 1600/2018 | Loss: 0.9654\n",
      "  Batch 1700/2018 | Loss: 1.2052\n",
      "  Batch 1800/2018 | Loss: 0.9534\n",
      "  Batch 1900/2018 | Loss: 0.7108\n",
      "  Batch 2000/2018 | Loss: 1.3268\n",
      "\n",
      "EPOCH 9/30 | Avg Loss: 0.8787\n",
      "\n",
      "Running full validation...\n",
      "Starting validation...\n",
      "  Validated 50/713 batches\n",
      "  Validated 100/713 batches\n",
      "  Validated 150/713 batches\n",
      "  Validated 200/713 batches\n",
      "  Validated 250/713 batches\n",
      "  Validated 300/713 batches\n",
      "  Validated 350/713 batches\n",
      "  Validated 400/713 batches\n",
      "  Validated 450/713 batches\n",
      "  Validated 500/713 batches\n",
      "  Validated 550/713 batches\n",
      "  Validated 600/713 batches\n",
      "  Validated 650/713 batches\n",
      "  Validated 700/713 batches\n",
      "\n",
      "Validation Results:\n",
      "  Total edit distance: 16348\n",
      "  Total sequence length: 41392\n",
      "  PER: 39.50%\n",
      "✓ NEW BEST MODEL SAVED!\n",
      "\n",
      "Sample predictions (phonemes):\n",
      "Sample 1:\n",
      "  Truth: Y UW  |  K AE N  |  S IY  |  DH AH  |  K OW D  |  AE T  |  DH IH S  |  P OY N T  |  AE Z  |  W EH L  | \n",
      "  Pred:  Y UW  |  K AE D  |  M IY  |  DH DH AH  |  K OW D  |  AE T  |  DH IH  |  P R T IH Z  |  W EH L  | \n",
      "Sample 2:\n",
      "  Truth: HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AA S T  |  D AW N  | \n",
      "  Pred:  HH AW  |  AH S  |  IH T  |  IY P  |  DH AH  |  AA S T  |  G AH  | \n",
      "Sample 3:\n",
      "  Truth: N AA T  |  T UW  |  K AA N T R AH V ER SH AH L  | \n",
      "  Pred:  N AA T  |  UW  |  AH F SH AH L  | \n",
      "------------------------------------------------------------\n",
      "  Batch 100/2018 | Loss: 1.0750\n",
      "  Batch 200/2018 | Loss: 0.6346\n",
      "  Batch 300/2018 | Loss: 0.5652\n",
      "  Batch 400/2018 | Loss: 0.7173\n",
      "  Batch 500/2018 | Loss: 0.7933\n",
      "  Batch 600/2018 | Loss: 0.6717\n",
      "  Batch 700/2018 | Loss: 0.7132\n",
      "  Batch 800/2018 | Loss: 0.7140\n",
      "  Batch 900/2018 | Loss: 1.1577\n",
      "  Batch 1000/2018 | Loss: 1.0123\n",
      "  Batch 1100/2018 | Loss: 0.2551\n",
      "  Batch 1200/2018 | Loss: 0.7940\n",
      "  Batch 1300/2018 | Loss: 1.0448\n",
      "  Batch 1400/2018 | Loss: 0.9611\n",
      "  Batch 1500/2018 | Loss: 0.6368\n",
      "  Batch 1600/2018 | Loss: 1.0673\n",
      "  Batch 1700/2018 | Loss: 0.6353\n",
      "  Batch 1800/2018 | Loss: 0.7353\n",
      "  Batch 1900/2018 | Loss: 0.9681\n",
      "  Batch 2000/2018 | Loss: 0.3332\n",
      "\n",
      "EPOCH 10/30 | Avg Loss: 0.7971\n",
      "  Batch 100/2018 | Loss: 0.6774\n",
      "  Batch 200/2018 | Loss: 0.7888\n",
      "  Batch 300/2018 | Loss: 0.6079\n",
      "  Batch 400/2018 | Loss: 0.8914\n",
      "  Batch 500/2018 | Loss: 0.8280\n",
      "  Batch 600/2018 | Loss: 0.4566\n",
      "  Batch 700/2018 | Loss: 1.0659\n",
      "  Batch 800/2018 | Loss: 0.7225\n",
      "  Batch 900/2018 | Loss: 0.6007\n",
      "  Batch 1000/2018 | Loss: 0.8352\n",
      "  Batch 1100/2018 | Loss: 0.6016\n",
      "  Batch 1200/2018 | Loss: 0.6555\n",
      "  Batch 1300/2018 | Loss: 0.7889\n",
      "  Batch 1400/2018 | Loss: 1.0416\n",
      "  Batch 1500/2018 | Loss: 1.2495\n",
      "  Batch 1600/2018 | Loss: 0.5395\n",
      "  Batch 1700/2018 | Loss: 0.7813\n",
      "  Batch 1800/2018 | Loss: 0.6453\n",
      "  Batch 1900/2018 | Loss: 0.6046\n",
      "  Batch 2000/2018 | Loss: 0.6129\n",
      "\n",
      "EPOCH 11/30 | Avg Loss: 0.7321\n",
      "  Batch 100/2018 | Loss: 0.3574\n",
      "  Batch 200/2018 | Loss: 0.8278\n",
      "  Batch 300/2018 | Loss: 0.4543\n",
      "  Batch 400/2018 | Loss: 0.5918\n",
      "  Batch 500/2018 | Loss: 0.2982\n",
      "  Batch 600/2018 | Loss: 0.4315\n",
      "  Batch 700/2018 | Loss: 0.7165\n",
      "  Batch 800/2018 | Loss: 0.5006\n",
      "  Batch 900/2018 | Loss: 0.7345\n",
      "  Batch 1000/2018 | Loss: 0.7008\n",
      "  Batch 1100/2018 | Loss: 0.7627\n",
      "  Batch 1200/2018 | Loss: 0.4896\n",
      "  Batch 1300/2018 | Loss: 0.6593\n",
      "  Batch 1400/2018 | Loss: 0.6139\n",
      "  Batch 1500/2018 | Loss: 0.6809\n",
      "  Batch 1600/2018 | Loss: 0.7841\n",
      "  Batch 1700/2018 | Loss: 0.5651\n",
      "  Batch 1800/2018 | Loss: 0.4249\n",
      "  Batch 1900/2018 | Loss: 0.8873\n",
      "  Batch 2000/2018 | Loss: 0.5182\n",
      "\n",
      "EPOCH 12/30 | Avg Loss: 0.6690\n",
      "\n",
      "Running full validation...\n",
      "Starting validation...\n",
      "  Validated 50/713 batches\n",
      "  Validated 100/713 batches\n",
      "  Validated 150/713 batches\n",
      "  Validated 200/713 batches\n",
      "  Validated 250/713 batches\n",
      "  Validated 300/713 batches\n",
      "  Validated 350/713 batches\n",
      "  Validated 400/713 batches\n",
      "  Validated 450/713 batches\n",
      "  Validated 500/713 batches\n",
      "  Validated 550/713 batches\n",
      "  Validated 600/713 batches\n",
      "  Validated 650/713 batches\n",
      "  Validated 700/713 batches\n",
      "\n",
      "Validation Results:\n",
      "  Total edit distance: 13286\n",
      "  Total sequence length: 41392\n",
      "  PER: 32.10%\n",
      "✓ NEW BEST MODEL SAVED!\n",
      "\n",
      "Sample predictions (phonemes):\n",
      "Sample 1:\n",
      "  Truth: Y UW  |  K AE N  |  S IY  |  DH AH  |  K OW D  |  AE T  |  DH IH S  |  P OY N T  |  AE Z  |  W EH L  | \n",
      "  Pred:  Y UW  |  K AE N  |  W IY  |  DH AH  |  K UH D  |  AE T  |  DH IH S  |  P OY N T IH S  |  W EY L  | \n",
      "Sample 2:\n",
      "  Truth: HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AA S T  |  D AW N  | \n",
      "  Pred:  HH AW  |  D AH Z  |  IH T  |  IY P  |  DH AH  |  K OW S T  |  G IH  | \n",
      "Sample 3:\n",
      "  Truth: N AA T  |  T UW  |  K AA N T R AH V ER SH AH L  | \n",
      "  Pred:  N AA T  |  Y UW  |  K AH T ER L SH L  |   | \n",
      "------------------------------------------------------------\n",
      "  Batch 100/2018 | Loss: 0.7967\n",
      "  Batch 200/2018 | Loss: 1.1722\n",
      "  Batch 300/2018 | Loss: 0.3851\n",
      "  Batch 400/2018 | Loss: 0.8047\n",
      "  Batch 500/2018 | Loss: 0.4512\n",
      "  Batch 600/2018 | Loss: 0.8519\n",
      "  Batch 700/2018 | Loss: 0.6405\n",
      "  Batch 800/2018 | Loss: 0.5611\n",
      "  Batch 900/2018 | Loss: 0.3726\n",
      "  Batch 1000/2018 | Loss: 0.4771\n",
      "  Batch 1100/2018 | Loss: 0.7160\n",
      "  Batch 1200/2018 | Loss: 0.6068\n",
      "  Batch 1300/2018 | Loss: 0.6726\n",
      "  Batch 1400/2018 | Loss: 0.8566\n",
      "  Batch 1500/2018 | Loss: 0.7085\n",
      "  Batch 1600/2018 | Loss: 0.7509\n",
      "  Batch 1700/2018 | Loss: 0.4063\n",
      "  Batch 1800/2018 | Loss: 0.5239\n",
      "  Batch 1900/2018 | Loss: 0.4652\n",
      "  Batch 2000/2018 | Loss: 0.7972\n",
      "\n",
      "EPOCH 13/30 | Avg Loss: 0.6115\n",
      "  Batch 100/2018 | Loss: 0.7771\n",
      "  Batch 200/2018 | Loss: 0.4259\n",
      "  Batch 300/2018 | Loss: 0.3998\n",
      "  Batch 400/2018 | Loss: 0.3318\n",
      "  Batch 500/2018 | Loss: 0.4912\n",
      "  Batch 600/2018 | Loss: 0.5256\n",
      "  Batch 700/2018 | Loss: 0.5377\n",
      "  Batch 800/2018 | Loss: 0.7626\n",
      "  Batch 900/2018 | Loss: 0.5709\n",
      "  Batch 1000/2018 | Loss: 0.3493\n",
      "  Batch 1100/2018 | Loss: 0.6472\n",
      "  Batch 1200/2018 | Loss: 0.4639\n",
      "  Batch 1300/2018 | Loss: 0.4101\n",
      "  Batch 1400/2018 | Loss: 0.6566\n",
      "  Batch 1500/2018 | Loss: 0.9585\n",
      "  Batch 1600/2018 | Loss: 0.2543\n",
      "  Batch 1700/2018 | Loss: 0.4452\n",
      "  Batch 1800/2018 | Loss: 0.7860\n",
      "  Batch 1900/2018 | Loss: 0.7661\n",
      "  Batch 2000/2018 | Loss: 0.6356\n",
      "\n",
      "EPOCH 14/30 | Avg Loss: 0.5537\n",
      "  Batch 100/2018 | Loss: 1.6164\n",
      "  Batch 200/2018 | Loss: 0.4828\n",
      "  Batch 300/2018 | Loss: 0.3640\n",
      "  Batch 400/2018 | Loss: 0.5988\n",
      "  Batch 500/2018 | Loss: 0.2638\n",
      "  Batch 600/2018 | Loss: 0.5204\n",
      "  Batch 700/2018 | Loss: 0.8752\n",
      "  Batch 800/2018 | Loss: 0.7269\n",
      "  Batch 900/2018 | Loss: 0.3404\n",
      "  Batch 1000/2018 | Loss: 0.5627\n",
      "  Batch 1100/2018 | Loss: 0.4244\n",
      "  Batch 1200/2018 | Loss: 0.7608\n",
      "  Batch 1300/2018 | Loss: 0.4832\n",
      "  Batch 1400/2018 | Loss: 0.5569\n",
      "  Batch 1500/2018 | Loss: 0.3638\n",
      "  Batch 1600/2018 | Loss: 0.6436\n",
      "  Batch 1700/2018 | Loss: 0.4387\n",
      "  Batch 1800/2018 | Loss: 0.4524\n",
      "  Batch 1900/2018 | Loss: 0.5053\n",
      "  Batch 2000/2018 | Loss: 0.4056\n",
      "\n",
      "EPOCH 15/30 | Avg Loss: 0.5074\n",
      "\n",
      "Running full validation...\n",
      "Starting validation...\n",
      "  Validated 50/713 batches\n",
      "  Validated 100/713 batches\n",
      "  Validated 150/713 batches\n",
      "  Validated 200/713 batches\n",
      "  Validated 250/713 batches\n",
      "  Validated 300/713 batches\n",
      "  Validated 350/713 batches\n",
      "  Validated 400/713 batches\n",
      "  Validated 450/713 batches\n",
      "  Validated 500/713 batches\n",
      "  Validated 550/713 batches\n",
      "  Validated 600/713 batches\n",
      "  Validated 650/713 batches\n",
      "  Validated 700/713 batches\n",
      "\n",
      "Validation Results:\n",
      "  Total edit distance: 12273\n",
      "  Total sequence length: 41392\n",
      "  PER: 29.65%\n",
      "✓ NEW BEST MODEL SAVED!\n",
      "\n",
      "Sample predictions (phonemes):\n",
      "Sample 1:\n",
      "  Truth: Y UW  |  K AE N  |  S IY  |  DH AH  |  K OW D  |  AE T  |  DH IH S  |  P OY N T  |  AE Z  |  W EH L  | \n",
      "  Pred:  Y UW  |  K AE N  |  W IY  |  DH AH  |  K UH M  |  AE T  |  DH IH S  |  P OY N T  |  G AE S Z  |  W EY L  | \n",
      "Sample 2:\n",
      "  Truth: HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AA S T  |  D AW N  | \n",
      "  Pred:  HH AW  |  D AH Z  |  IH T T  |  K IY P  |  DH AH  |  K AA S  |  G EH  | \n",
      "Sample 3:\n",
      "  Truth: N AA T  |  T UW  |  K AA N T R AH V ER SH AH L  | \n",
      "  Pred:  N AA T  |  T UW  |  K AH K N T R UW EY SH SH AH L  | \n",
      "------------------------------------------------------------\n",
      "  Batch 100/2018 | Loss: 0.1612\n",
      "  Batch 200/2018 | Loss: 0.4104\n",
      "  Batch 300/2018 | Loss: 0.4543\n",
      "  Batch 400/2018 | Loss: 0.3511\n",
      "  Batch 500/2018 | Loss: 0.4687\n",
      "  Batch 600/2018 | Loss: 0.6512\n",
      "  Batch 700/2018 | Loss: 0.6476\n",
      "  Batch 800/2018 | Loss: 0.4481\n",
      "  Batch 900/2018 | Loss: 0.7146\n",
      "  Batch 1000/2018 | Loss: 0.4350\n",
      "  Batch 1100/2018 | Loss: 0.4200\n",
      "  Batch 1200/2018 | Loss: 0.4840\n",
      "  Batch 1300/2018 | Loss: 0.3005\n",
      "  Batch 1400/2018 | Loss: 0.4775\n",
      "  Batch 1500/2018 | Loss: 0.5219\n",
      "  Batch 1600/2018 | Loss: 0.9245\n",
      "  Batch 1700/2018 | Loss: 0.4690\n",
      "  Batch 1800/2018 | Loss: 0.4803\n",
      "  Batch 1900/2018 | Loss: 0.2559\n",
      "  Batch 2000/2018 | Loss: 0.2517\n",
      "\n",
      "EPOCH 16/30 | Avg Loss: 0.4639\n",
      "  Batch 100/2018 | Loss: 0.0741\n",
      "  Batch 200/2018 | Loss: 0.6875\n",
      "  Batch 300/2018 | Loss: 0.7481\n",
      "  Batch 400/2018 | Loss: 0.5934\n",
      "  Batch 500/2018 | Loss: 0.3542\n",
      "  Batch 600/2018 | Loss: 0.2300\n",
      "  Batch 700/2018 | Loss: 0.3480\n",
      "  Batch 800/2018 | Loss: 0.5367\n",
      "  Batch 900/2018 | Loss: 0.6017\n",
      "  Batch 1000/2018 | Loss: 0.2173\n",
      "  Batch 1100/2018 | Loss: 0.3324\n",
      "  Batch 1200/2018 | Loss: 0.2166\n",
      "  Batch 1300/2018 | Loss: 0.3495\n",
      "  Batch 1400/2018 | Loss: 0.3435\n",
      "  Batch 1500/2018 | Loss: 0.3313\n",
      "  Batch 1600/2018 | Loss: 0.1982\n",
      "  Batch 1700/2018 | Loss: 0.7921\n",
      "  Batch 1800/2018 | Loss: 0.4371\n",
      "  Batch 1900/2018 | Loss: 0.3586\n",
      "  Batch 2000/2018 | Loss: 0.2458\n",
      "\n",
      "EPOCH 17/30 | Avg Loss: 0.4199\n",
      "  Batch 100/2018 | Loss: 0.1386\n",
      "  Batch 200/2018 | Loss: 0.5141\n",
      "  Batch 300/2018 | Loss: 0.3584\n",
      "  Batch 400/2018 | Loss: 1.4715\n",
      "  Batch 500/2018 | Loss: 0.4438\n",
      "  Batch 600/2018 | Loss: 0.3338\n",
      "  Batch 700/2018 | Loss: 0.4724\n",
      "  Batch 800/2018 | Loss: 0.5592\n",
      "  Batch 900/2018 | Loss: 0.3853\n",
      "  Batch 1000/2018 | Loss: 0.3672\n",
      "  Batch 1100/2018 | Loss: 0.2787\n",
      "  Batch 1200/2018 | Loss: 0.2250\n",
      "  Batch 1300/2018 | Loss: 0.3093\n",
      "  Batch 1400/2018 | Loss: 0.3206\n",
      "  Batch 1500/2018 | Loss: 0.3632\n",
      "  Batch 1600/2018 | Loss: 0.3025\n",
      "  Batch 1700/2018 | Loss: 0.2362\n",
      "  Batch 1800/2018 | Loss: 0.3909\n",
      "  Batch 1900/2018 | Loss: 0.2034\n",
      "  Batch 2000/2018 | Loss: 0.6183\n",
      "\n",
      "EPOCH 18/30 | Avg Loss: 0.3764\n",
      "\n",
      "Running full validation...\n",
      "Starting validation...\n",
      "  Validated 50/713 batches\n",
      "  Validated 100/713 batches\n",
      "  Validated 150/713 batches\n",
      "  Validated 200/713 batches\n",
      "  Validated 250/713 batches\n",
      "  Validated 300/713 batches\n",
      "  Validated 350/713 batches\n",
      "  Validated 400/713 batches\n",
      "  Validated 450/713 batches\n",
      "  Validated 500/713 batches\n",
      "  Validated 550/713 batches\n",
      "  Validated 600/713 batches\n",
      "  Validated 650/713 batches\n",
      "  Validated 700/713 batches\n",
      "\n",
      "Validation Results:\n",
      "  Total edit distance: 11116\n",
      "  Total sequence length: 41392\n",
      "  PER: 26.86%\n",
      "✓ NEW BEST MODEL SAVED!\n",
      "\n",
      "Sample predictions (phonemes):\n",
      "Sample 1:\n",
      "  Truth: Y UW  |  K AE N  |  S IY  |  DH AH  |  K OW D  |  AE T  |  DH IH S  |  P OY N T  |  AE Z  |  W EH L  | \n",
      "  Pred:  Y UW  |  K AE N  |  S IY  |  DH AH  |  K UH D  |  AE T  |  DH IH S  |  P OY N T  |  AH S  |  W EY  | \n",
      "Sample 2:\n",
      "  Truth: HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AA S T  |  D AW N  | \n",
      "  Pred:  HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH S  |  K AH S T  |  G AE D  |   | \n",
      "Sample 3:\n",
      "  Truth: N AA T  |  T UW  |  K AA N T R AH V ER SH AH L  | \n",
      "  Pred:  N AA T  |  T UW  |  K AH T SH SH AH L  | \n",
      "------------------------------------------------------------\n",
      "  Batch 100/2018 | Loss: 1.1137\n",
      "  Batch 200/2018 | Loss: 0.3403\n",
      "  Batch 300/2018 | Loss: 0.1856\n",
      "  Batch 400/2018 | Loss: 0.4846\n",
      "  Batch 500/2018 | Loss: 0.2097\n",
      "  Batch 600/2018 | Loss: 0.2015\n",
      "  Batch 700/2018 | Loss: 0.4614\n",
      "  Batch 800/2018 | Loss: 0.4187\n",
      "  Batch 900/2018 | Loss: 0.3209\n",
      "  Batch 1000/2018 | Loss: 0.2397\n",
      "  Batch 1100/2018 | Loss: 0.6590\n",
      "  Batch 1200/2018 | Loss: 0.2755\n",
      "  Batch 1300/2018 | Loss: 0.2487\n",
      "  Batch 1400/2018 | Loss: 0.3393\n",
      "  Batch 1500/2018 | Loss: 0.3441\n",
      "  Batch 1600/2018 | Loss: 0.2045\n",
      "  Batch 1700/2018 | Loss: 0.2611\n",
      "  Batch 1800/2018 | Loss: 0.5122\n",
      "  Batch 1900/2018 | Loss: 0.3680\n",
      "  Batch 2000/2018 | Loss: 0.3465\n",
      "\n",
      "EPOCH 19/30 | Avg Loss: 0.3398\n",
      "  Batch 100/2018 | Loss: 0.2842\n",
      "  Batch 200/2018 | Loss: 0.6918\n",
      "  Batch 300/2018 | Loss: 0.2099\n",
      "  Batch 400/2018 | Loss: 0.1914\n",
      "  Batch 500/2018 | Loss: 0.3871\n",
      "  Batch 600/2018 | Loss: 0.3368\n",
      "  Batch 700/2018 | Loss: 0.3757\n",
      "  Batch 800/2018 | Loss: 0.3445\n",
      "  Batch 900/2018 | Loss: 0.2134\n",
      "  Batch 1000/2018 | Loss: 0.3896\n",
      "  Batch 1100/2018 | Loss: 0.3032\n",
      "  Batch 1200/2018 | Loss: 0.0987\n",
      "  Batch 1300/2018 | Loss: 0.2500\n",
      "  Batch 1400/2018 | Loss: 0.2208\n",
      "  Batch 1500/2018 | Loss: 0.6938\n",
      "  Batch 1600/2018 | Loss: 0.2575\n",
      "  Batch 1700/2018 | Loss: 0.2326\n",
      "  Batch 1800/2018 | Loss: 0.2339\n",
      "  Batch 1900/2018 | Loss: 0.2885\n",
      "  Batch 2000/2018 | Loss: 0.3778\n",
      "\n",
      "EPOCH 20/30 | Avg Loss: 0.2987\n",
      "  Batch 100/2018 | Loss: 0.2058\n",
      "  Batch 200/2018 | Loss: 0.2985\n",
      "  Batch 300/2018 | Loss: 0.5251\n",
      "  Batch 400/2018 | Loss: 0.3124\n",
      "  Batch 500/2018 | Loss: 0.3235\n",
      "  Batch 600/2018 | Loss: 0.1087\n",
      "  Batch 700/2018 | Loss: 0.0530\n",
      "  Batch 800/2018 | Loss: 0.2712\n",
      "  Batch 900/2018 | Loss: 0.3383\n",
      "  Batch 1000/2018 | Loss: 0.3491\n",
      "  Batch 1100/2018 | Loss: 0.3051\n",
      "  Batch 1200/2018 | Loss: 0.1510\n",
      "  Batch 1300/2018 | Loss: 0.2681\n",
      "  Batch 1400/2018 | Loss: 0.5468\n",
      "  Batch 1500/2018 | Loss: 0.1414\n",
      "  Batch 1600/2018 | Loss: 0.2829\n",
      "  Batch 1700/2018 | Loss: 0.3831\n",
      "  Batch 1800/2018 | Loss: 0.2515\n",
      "  Batch 1900/2018 | Loss: 0.1152\n",
      "  Batch 2000/2018 | Loss: 0.1558\n",
      "\n",
      "EPOCH 21/30 | Avg Loss: 0.2682\n",
      "\n",
      "Running full validation...\n",
      "Starting validation...\n",
      "  Validated 50/713 batches\n",
      "  Validated 100/713 batches\n",
      "  Validated 150/713 batches\n",
      "  Validated 200/713 batches\n",
      "  Validated 250/713 batches\n",
      "  Validated 300/713 batches\n",
      "  Validated 350/713 batches\n",
      "  Validated 400/713 batches\n",
      "  Validated 450/713 batches\n",
      "  Validated 500/713 batches\n",
      "  Validated 550/713 batches\n",
      "  Validated 600/713 batches\n",
      "  Validated 650/713 batches\n",
      "  Validated 700/713 batches\n",
      "\n",
      "Validation Results:\n",
      "  Total edit distance: 10886\n",
      "  Total sequence length: 41392\n",
      "  PER: 26.30%\n",
      "✓ NEW BEST MODEL SAVED!\n",
      "\n",
      "Sample predictions (phonemes):\n",
      "Sample 1:\n",
      "  Truth: Y UW  |  K AE N  |  S IY  |  DH AH  |  K OW D  |  AE T  |  DH IH S  |  P OY N T  |  AE Z  |  W EH L  | \n",
      "  Pred:  Y UW  |  K AE N  |  S IY  |  DH AH  |  K UH D  |  AE T  |  DH IH S  |  P OY N T  |  AE Z  |  W IY L  | \n",
      "Sample 2:\n",
      "  Truth: HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AA S T  |  D AW N  | \n",
      "  Pred:  HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AA S T  |  IH N  |   | \n",
      "Sample 3:\n",
      "  Truth: N AA T  |  T UW  |  K AA N T R AH V ER SH AH L  | \n",
      "  Pred:  N AA T  |  T UW  |  K AH N AH UW SH AH L  |   | \n",
      "------------------------------------------------------------\n",
      "  Batch 100/2018 | Loss: 0.2053\n",
      "  Batch 200/2018 | Loss: 0.2735\n",
      "  Batch 300/2018 | Loss: 0.1637\n",
      "  Batch 400/2018 | Loss: 0.1221\n",
      "  Batch 500/2018 | Loss: 0.1279\n",
      "  Batch 600/2018 | Loss: 0.1949\n",
      "  Batch 700/2018 | Loss: 0.1651\n",
      "  Batch 800/2018 | Loss: 0.0929\n",
      "  Batch 900/2018 | Loss: 0.0477\n",
      "  Batch 1000/2018 | Loss: 0.2873\n",
      "  Batch 1100/2018 | Loss: 0.0893\n",
      "  Batch 1200/2018 | Loss: 0.2279\n",
      "  Batch 1300/2018 | Loss: 0.2712\n",
      "  Batch 1400/2018 | Loss: 0.3426\n",
      "  Batch 1500/2018 | Loss: 0.2379\n",
      "  Batch 1600/2018 | Loss: 0.0394\n",
      "  Batch 1700/2018 | Loss: 0.1964\n",
      "  Batch 1800/2018 | Loss: 0.2485\n",
      "  Batch 1900/2018 | Loss: 0.2613\n",
      "  Batch 2000/2018 | Loss: 0.1544\n",
      "\n",
      "EPOCH 22/30 | Avg Loss: 0.2389\n",
      "  Batch 100/2018 | Loss: 0.1090\n",
      "  Batch 200/2018 | Loss: 0.2848\n",
      "  Batch 300/2018 | Loss: 0.0733\n",
      "  Batch 400/2018 | Loss: 0.1982\n",
      "  Batch 500/2018 | Loss: 0.1215\n",
      "  Batch 600/2018 | Loss: 0.1686\n",
      "  Batch 700/2018 | Loss: 0.1713\n",
      "  Batch 800/2018 | Loss: 0.1574\n",
      "  Batch 900/2018 | Loss: 0.1116\n",
      "  Batch 1000/2018 | Loss: 0.1019\n",
      "  Batch 1100/2018 | Loss: 0.2971\n",
      "  Batch 1200/2018 | Loss: 0.3226\n",
      "  Batch 1300/2018 | Loss: 0.2653\n",
      "  Batch 1400/2018 | Loss: 0.1101\n",
      "  Batch 1500/2018 | Loss: 0.2027\n",
      "  Batch 1600/2018 | Loss: 0.2785\n",
      "  Batch 1700/2018 | Loss: 0.1635\n",
      "  Batch 1800/2018 | Loss: 0.1946\n",
      "  Batch 1900/2018 | Loss: 0.0635\n",
      "  Batch 2000/2018 | Loss: 0.3187\n",
      "\n",
      "EPOCH 23/30 | Avg Loss: 0.2086\n",
      "  Batch 100/2018 | Loss: 0.0874\n",
      "  Batch 200/2018 | Loss: 0.2581\n",
      "  Batch 300/2018 | Loss: 0.1126\n",
      "  Batch 400/2018 | Loss: 0.1508\n",
      "  Batch 500/2018 | Loss: 0.1808\n",
      "  Batch 600/2018 | Loss: 0.3461\n",
      "  Batch 700/2018 | Loss: 0.1577\n",
      "  Batch 800/2018 | Loss: 0.1014\n",
      "  Batch 900/2018 | Loss: 0.1942\n",
      "  Batch 1000/2018 | Loss: 0.0250\n",
      "  Batch 1100/2018 | Loss: 0.1389\n",
      "  Batch 1200/2018 | Loss: 0.2580\n",
      "  Batch 1300/2018 | Loss: 0.2133\n",
      "  Batch 1400/2018 | Loss: 0.0352\n",
      "  Batch 1500/2018 | Loss: 0.1819\n",
      "  Batch 1600/2018 | Loss: 0.3269\n",
      "  Batch 1700/2018 | Loss: 0.0974\n",
      "  Batch 1800/2018 | Loss: 0.1057\n",
      "  Batch 1900/2018 | Loss: 0.0451\n",
      "  Batch 2000/2018 | Loss: 0.1957\n",
      "\n",
      "EPOCH 24/30 | Avg Loss: 0.1861\n",
      "\n",
      "Running full validation...\n",
      "Starting validation...\n",
      "  Validated 50/713 batches\n",
      "  Validated 100/713 batches\n",
      "  Validated 150/713 batches\n",
      "  Validated 200/713 batches\n",
      "  Validated 250/713 batches\n",
      "  Validated 300/713 batches\n",
      "  Validated 350/713 batches\n",
      "  Validated 400/713 batches\n",
      "  Validated 450/713 batches\n",
      "  Validated 500/713 batches\n",
      "  Validated 550/713 batches\n",
      "  Validated 600/713 batches\n",
      "  Validated 650/713 batches\n",
      "  Validated 700/713 batches\n",
      "\n",
      "Validation Results:\n",
      "  Total edit distance: 10296\n",
      "  Total sequence length: 41392\n",
      "  PER: 24.87%\n",
      "✓ NEW BEST MODEL SAVED!\n",
      "\n",
      "Sample predictions (phonemes):\n",
      "Sample 1:\n",
      "  Truth: Y UW  |  K AE N  |  S IY  |  DH AH  |  K OW D  |  AE T  |  DH IH S  |  P OY N T  |  AE Z  |  W EH L  | \n",
      "  Pred:  Y UW  |  K AE N  |  S IY  |  DH AH  |  K UH M D  |  AE T  |  DH IH S  |  P OY N T  |  AH S  |  W EY L  | \n",
      "Sample 2:\n",
      "  Truth: HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AA S T  |  D AW N  | \n",
      "  Pred:  HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AH S T  |  AE  |   | \n",
      "Sample 3:\n",
      "  Truth: N AA T  |  T UW  |  K AA N T R AH V ER SH AH L  | \n",
      "  Pred:  N AA T  |  T UW  |  K AE N T AH SH AH L  |   | \n",
      "------------------------------------------------------------\n",
      "  Batch 100/2018 | Loss: 0.1407\n",
      "  Batch 200/2018 | Loss: 0.1188\n",
      "  Batch 300/2018 | Loss: 0.1820\n",
      "  Batch 400/2018 | Loss: 0.1748\n",
      "  Batch 500/2018 | Loss: 0.1469\n",
      "  Batch 600/2018 | Loss: 0.1652\n",
      "  Batch 700/2018 | Loss: 0.2108\n",
      "  Batch 800/2018 | Loss: 0.1745\n",
      "  Batch 900/2018 | Loss: 0.2580\n",
      "  Batch 1000/2018 | Loss: 0.1358\n",
      "  Batch 1100/2018 | Loss: 0.4691\n",
      "  Batch 1200/2018 | Loss: 0.1418\n",
      "  Batch 1300/2018 | Loss: 0.2129\n",
      "  Batch 1400/2018 | Loss: 0.2405\n",
      "  Batch 1500/2018 | Loss: 0.2901\n",
      "  Batch 1600/2018 | Loss: 0.1357\n",
      "  Batch 1700/2018 | Loss: 0.3101\n",
      "  Batch 1800/2018 | Loss: 0.0909\n",
      "  Batch 1900/2018 | Loss: 0.1847\n",
      "  Batch 2000/2018 | Loss: 0.1729\n",
      "\n",
      "EPOCH 25/30 | Avg Loss: 0.1684\n",
      "  Batch 100/2018 | Loss: 0.3569\n",
      "  Batch 200/2018 | Loss: 0.2467\n",
      "  Batch 300/2018 | Loss: 0.0958\n",
      "  Batch 400/2018 | Loss: 0.0519\n",
      "  Batch 500/2018 | Loss: 0.0446\n",
      "  Batch 600/2018 | Loss: 0.0561\n",
      "  Batch 700/2018 | Loss: 0.0835\n",
      "  Batch 800/2018 | Loss: 0.1827\n",
      "  Batch 900/2018 | Loss: 0.0937\n",
      "  Batch 1000/2018 | Loss: 0.3172\n",
      "  Batch 1100/2018 | Loss: 0.0907\n",
      "  Batch 1200/2018 | Loss: 0.0258\n",
      "  Batch 1300/2018 | Loss: 0.0452\n",
      "  Batch 1400/2018 | Loss: 0.0585\n",
      "  Batch 1500/2018 | Loss: 0.0424\n",
      "  Batch 1600/2018 | Loss: 0.2227\n",
      "  Batch 1700/2018 | Loss: 0.2149\n",
      "  Batch 1800/2018 | Loss: 0.1241\n",
      "  Batch 1900/2018 | Loss: 0.0910\n",
      "  Batch 2000/2018 | Loss: 0.2109\n",
      "\n",
      "EPOCH 26/30 | Avg Loss: 0.1525\n",
      "\n",
      "Running full validation...\n",
      "Starting validation...\n",
      "  Validated 50/713 batches\n",
      "  Validated 100/713 batches\n",
      "  Validated 150/713 batches\n",
      "  Validated 200/713 batches\n",
      "  Validated 250/713 batches\n",
      "  Validated 300/713 batches\n",
      "  Validated 350/713 batches\n",
      "  Validated 400/713 batches\n",
      "  Validated 450/713 batches\n",
      "  Validated 500/713 batches\n",
      "  Validated 550/713 batches\n",
      "  Validated 600/713 batches\n",
      "  Validated 650/713 batches\n",
      "  Validated 700/713 batches\n",
      "\n",
      "Validation Results:\n",
      "  Total edit distance: 10055\n",
      "  Total sequence length: 41392\n",
      "  PER: 24.29%\n",
      "✓ NEW BEST MODEL SAVED!\n",
      "\n",
      "Sample predictions (phonemes):\n",
      "Sample 1:\n",
      "  Truth: Y UW  |  K AE N  |  S IY  |  DH AH  |  K OW D  |  AE T  |  DH IH S  |  P OY N T  |  AE Z  |  W EH L  | \n",
      "  Pred:  Y UW  |  K AE N  |  S IY  |  DH AH  |  K UH D  |  AE T  |  DH IH S  |  P OY N T  |  AH S  |  W EY L  | \n",
      "Sample 2:\n",
      "  Truth: HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AA S T  |  D AW N  | \n",
      "  Pred:  HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AH S T  |  AE N  |   | \n",
      "Sample 3:\n",
      "  Truth: N AA T  |  T UW  |  K AA N T R AH V ER SH AH L  | \n",
      "  Pred:  N AA T  |  T UW  |  K AH N AH SH AH L  | \n",
      "------------------------------------------------------------\n",
      "  Batch 100/2018 | Loss: 0.0722\n",
      "  Batch 200/2018 | Loss: 0.1544\n",
      "  Batch 300/2018 | Loss: 0.2599\n",
      "  Batch 400/2018 | Loss: 0.1278\n",
      "  Batch 500/2018 | Loss: 0.1385\n",
      "  Batch 600/2018 | Loss: 0.1228\n",
      "  Batch 700/2018 | Loss: 0.2218\n",
      "  Batch 800/2018 | Loss: 0.1106\n",
      "  Batch 900/2018 | Loss: 0.2003\n",
      "  Batch 1000/2018 | Loss: 0.0827\n",
      "  Batch 1100/2018 | Loss: 0.1431\n",
      "  Batch 1200/2018 | Loss: 0.1269\n",
      "  Batch 1300/2018 | Loss: 0.0558\n",
      "  Batch 1400/2018 | Loss: 0.1357\n",
      "  Batch 1500/2018 | Loss: 0.0721\n",
      "  Batch 1600/2018 | Loss: 0.0739\n",
      "  Batch 1700/2018 | Loss: 0.1203\n",
      "  Batch 1800/2018 | Loss: 0.0775\n",
      "  Batch 1900/2018 | Loss: 0.2148\n",
      "  Batch 2000/2018 | Loss: 0.0458\n",
      "\n",
      "EPOCH 27/30 | Avg Loss: 0.1417\n",
      "\n",
      "Running full validation...\n",
      "Starting validation...\n",
      "  Validated 50/713 batches\n",
      "  Validated 100/713 batches\n",
      "  Validated 150/713 batches\n",
      "  Validated 200/713 batches\n",
      "  Validated 250/713 batches\n",
      "  Validated 300/713 batches\n",
      "  Validated 350/713 batches\n",
      "  Validated 400/713 batches\n",
      "  Validated 450/713 batches\n",
      "  Validated 500/713 batches\n",
      "  Validated 550/713 batches\n",
      "  Validated 600/713 batches\n",
      "  Validated 650/713 batches\n",
      "  Validated 700/713 batches\n",
      "\n",
      "Validation Results:\n",
      "  Total edit distance: 9969\n",
      "  Total sequence length: 41392\n",
      "  PER: 24.08%\n",
      "✓ NEW BEST MODEL SAVED!\n",
      "\n",
      "Sample predictions (phonemes):\n",
      "Sample 1:\n",
      "  Truth: Y UW  |  K AE N  |  S IY  |  DH AH  |  K OW D  |  AE T  |  DH IH S  |  P OY N T  |  AE Z  |  W EH L  | \n",
      "  Pred:  Y UW  |  K AE N  |  IY  |  DH AH  |  K UH D  |  AE T  |  DH IH S  |  P OY T  |  AE S  |  W EY L  | \n",
      "Sample 2:\n",
      "  Truth: HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AA S T  |  D AW N  | \n",
      "  Pred:  HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AH S T  |  AE N  |   | \n",
      "Sample 3:\n",
      "  Truth: N AA T  |  T UW  |  K AA N T R AH V ER SH AH L  | \n",
      "  Pred:  N AA T  |  T UW  |  K AH N AH SH AH L  | \n",
      "------------------------------------------------------------\n",
      "  Batch 100/2018 | Loss: 0.0922\n",
      "  Batch 200/2018 | Loss: 0.2094\n",
      "  Batch 300/2018 | Loss: 0.1209\n",
      "  Batch 400/2018 | Loss: 0.1130\n",
      "  Batch 500/2018 | Loss: 0.1369\n",
      "  Batch 600/2018 | Loss: 0.0628\n",
      "  Batch 700/2018 | Loss: 0.1279\n",
      "  Batch 800/2018 | Loss: 0.1611\n",
      "  Batch 900/2018 | Loss: 0.0416\n",
      "  Batch 1000/2018 | Loss: 0.1655\n",
      "  Batch 1100/2018 | Loss: 0.1775\n",
      "  Batch 1200/2018 | Loss: 0.1259\n",
      "  Batch 1300/2018 | Loss: 0.2061\n",
      "  Batch 1400/2018 | Loss: 0.1516\n",
      "  Batch 1500/2018 | Loss: 0.1483\n",
      "  Batch 1600/2018 | Loss: 0.0636\n",
      "  Batch 1700/2018 | Loss: 0.1705\n",
      "  Batch 1800/2018 | Loss: 0.2146\n",
      "  Batch 1900/2018 | Loss: 0.3101\n",
      "  Batch 2000/2018 | Loss: 0.1721\n",
      "\n",
      "EPOCH 28/30 | Avg Loss: 0.1336\n",
      "\n",
      "Running full validation...\n",
      "Starting validation...\n",
      "  Validated 50/713 batches\n",
      "  Validated 100/713 batches\n",
      "  Validated 150/713 batches\n",
      "  Validated 200/713 batches\n",
      "  Validated 250/713 batches\n",
      "  Validated 300/713 batches\n",
      "  Validated 350/713 batches\n",
      "  Validated 400/713 batches\n",
      "  Validated 450/713 batches\n",
      "  Validated 500/713 batches\n",
      "  Validated 550/713 batches\n",
      "  Validated 600/713 batches\n",
      "  Validated 650/713 batches\n",
      "  Validated 700/713 batches\n",
      "\n",
      "Validation Results:\n",
      "  Total edit distance: 9938\n",
      "  Total sequence length: 41392\n",
      "  PER: 24.01%\n",
      "✓ NEW BEST MODEL SAVED!\n",
      "\n",
      "Sample predictions (phonemes):\n",
      "Sample 1:\n",
      "  Truth: Y UW  |  K AE N  |  S IY  |  DH AH  |  K OW D  |  AE T  |  DH IH S  |  P OY N T  |  AE Z  |  W EH L  | \n",
      "  Pred:  Y UW  |  K AE N  |  S IY  |  DH AH  |  K UH D  |  AE T  |  DH IH S  |  P OY T  |  AE S  |  W EY L  | \n",
      "Sample 2:\n",
      "  Truth: HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AA S T  |  D AW N  | \n",
      "  Pred:  HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AH S T  |  AE N  |   | \n",
      "Sample 3:\n",
      "  Truth: N AA T  |  T UW  |  K AA N T R AH V ER SH AH L  | \n",
      "  Pred:  N AA T  |  T UW  |  K AH N AH SH AH L  | \n",
      "------------------------------------------------------------\n",
      "  Batch 100/2018 | Loss: 0.2976\n",
      "  Batch 200/2018 | Loss: 0.0876\n",
      "  Batch 300/2018 | Loss: 0.3110\n",
      "  Batch 400/2018 | Loss: 0.0590\n",
      "  Batch 500/2018 | Loss: 0.0133\n",
      "  Batch 600/2018 | Loss: 0.1278\n",
      "  Batch 700/2018 | Loss: 0.3267\n",
      "  Batch 800/2018 | Loss: 0.2221\n",
      "  Batch 900/2018 | Loss: 0.0730\n",
      "  Batch 1000/2018 | Loss: 0.0602\n",
      "  Batch 1100/2018 | Loss: 0.0883\n",
      "  Batch 1200/2018 | Loss: 0.0988\n",
      "  Batch 1300/2018 | Loss: 0.1182\n",
      "  Batch 1400/2018 | Loss: 0.0977\n",
      "  Batch 1500/2018 | Loss: 0.2395\n",
      "  Batch 1600/2018 | Loss: 0.2583\n",
      "  Batch 1700/2018 | Loss: 0.0444\n",
      "  Batch 1800/2018 | Loss: 0.2212\n",
      "  Batch 1900/2018 | Loss: 0.1629\n",
      "  Batch 2000/2018 | Loss: 0.2506\n",
      "\n",
      "EPOCH 29/30 | Avg Loss: 0.1293\n",
      "\n",
      "Running full validation...\n",
      "Starting validation...\n",
      "  Validated 50/713 batches\n",
      "  Validated 100/713 batches\n",
      "  Validated 150/713 batches\n",
      "  Validated 200/713 batches\n",
      "  Validated 250/713 batches\n",
      "  Validated 300/713 batches\n",
      "  Validated 350/713 batches\n",
      "  Validated 400/713 batches\n",
      "  Validated 450/713 batches\n",
      "  Validated 500/713 batches\n",
      "  Validated 550/713 batches\n",
      "  Validated 600/713 batches\n",
      "  Validated 650/713 batches\n",
      "  Validated 700/713 batches\n",
      "\n",
      "Validation Results:\n",
      "  Total edit distance: 9895\n",
      "  Total sequence length: 41392\n",
      "  PER: 23.91%\n",
      "✓ NEW BEST MODEL SAVED!\n",
      "\n",
      "Sample predictions (phonemes):\n",
      "Sample 1:\n",
      "  Truth: Y UW  |  K AE N  |  S IY  |  DH AH  |  K OW D  |  AE T  |  DH IH S  |  P OY N T  |  AE Z  |  W EH L  | \n",
      "  Pred:  Y UW  |  K AE N  |  S IY  |  DH AH  |  K UH D  |  AE T  |  DH IH S  |  P OY N T  |  AE S  |  W EY L  | \n",
      "Sample 2:\n",
      "  Truth: HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AA S T  |  D AW N  | \n",
      "  Pred:  HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AH S T  |  AE N  |   | \n",
      "Sample 3:\n",
      "  Truth: N AA T  |  T UW  |  K AA N T R AH V ER SH AH L  | \n",
      "  Pred:  N AA T  |  T UW  |  K AH N AH SH AH L  | \n",
      "------------------------------------------------------------\n",
      "  Batch 100/2018 | Loss: 0.2742\n",
      "  Batch 200/2018 | Loss: 0.0944\n",
      "  Batch 300/2018 | Loss: 0.1362\n",
      "  Batch 400/2018 | Loss: 0.2468\n",
      "  Batch 500/2018 | Loss: 0.0457\n",
      "  Batch 600/2018 | Loss: 0.1873\n",
      "  Batch 700/2018 | Loss: 0.2409\n",
      "  Batch 800/2018 | Loss: 0.0582\n",
      "  Batch 900/2018 | Loss: 0.0445\n",
      "  Batch 1000/2018 | Loss: 0.1002\n",
      "  Batch 1100/2018 | Loss: 0.0689\n",
      "  Batch 1200/2018 | Loss: 0.0610\n",
      "  Batch 1300/2018 | Loss: 0.0990\n",
      "  Batch 1400/2018 | Loss: 0.1244\n",
      "  Batch 1500/2018 | Loss: 0.1809\n",
      "  Batch 1600/2018 | Loss: 0.0967\n",
      "  Batch 1700/2018 | Loss: 0.0573\n",
      "  Batch 1800/2018 | Loss: 0.2562\n",
      "  Batch 1900/2018 | Loss: 0.1397\n",
      "  Batch 2000/2018 | Loss: 0.2107\n",
      "\n",
      "EPOCH 30/30 | Avg Loss: 0.1278\n",
      "\n",
      "Running full validation...\n",
      "Starting validation...\n",
      "  Validated 50/713 batches\n",
      "  Validated 100/713 batches\n",
      "  Validated 150/713 batches\n",
      "  Validated 200/713 batches\n",
      "  Validated 250/713 batches\n",
      "  Validated 300/713 batches\n",
      "  Validated 350/713 batches\n",
      "  Validated 400/713 batches\n",
      "  Validated 450/713 batches\n",
      "  Validated 500/713 batches\n",
      "  Validated 550/713 batches\n",
      "  Validated 600/713 batches\n",
      "  Validated 650/713 batches\n",
      "  Validated 700/713 batches\n",
      "\n",
      "Validation Results:\n",
      "  Total edit distance: 9891\n",
      "  Total sequence length: 41392\n",
      "  PER: 23.90%\n",
      "✓ NEW BEST MODEL SAVED!\n",
      "\n",
      "Sample predictions (phonemes):\n",
      "Sample 1:\n",
      "  Truth: Y UW  |  K AE N  |  S IY  |  DH AH  |  K OW D  |  AE T  |  DH IH S  |  P OY N T  |  AE Z  |  W EH L  | \n",
      "  Pred:  Y UW  |  K AE N  |  S IY  |  DH AH  |  K UH D  |  AE T  |  DH IH S  |  P OY N T  |  AE S  |  W EY L  | \n",
      "Sample 2:\n",
      "  Truth: HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AA S T  |  D AW N  | \n",
      "  Pred:  HH AW  |  D AH Z  |  IH T  |  K IY P  |  DH AH  |  K AH S T  |  AE N  |   | \n",
      "Sample 3:\n",
      "  Truth: N AA T  |  T UW  |  K AA N T R AH V ER SH AH L  | \n",
      "  Pred:  N AA T  |  T UW  |  K AH N AH SH AH L  | \n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Training complete!\n",
      "Best PER: 23.90%\n",
      "Model saved: best_model_full_per.pt\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Brain-to-Text CTC Training - Full Dataset + PER\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import editdistance\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --------------------------\n",
    "# CONFIG\n",
    "# --------------------------\n",
    "BASE_PATH = \"/kaggle/input/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final\"\n",
    "BATCH_SIZE = 4  # Reduced for full dataset to avoid OOM\n",
    "BATCH_SIZE_VAL = 2\n",
    "NUM_WORKERS = 2\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "SAVE_PATH = \"best_model_full_per.pt\"\n",
    "\n",
    "# Model architecture\n",
    "D_MODEL = 256\n",
    "NHEAD = 8\n",
    "NUM_LAYERS = 4\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# NO downsampling, NO max sequence length limit\n",
    "DOWNSAMPLE_FACTOR = 1  # No downsampling\n",
    "MAX_SEQ_LEN = None  # No length cap\n",
    "\n",
    "# Use ALL data - no subset limits\n",
    "MAX_TRAIN_TRIALS = None  # Use all training trials\n",
    "MAX_VAL_TRIALS = None  # Use all validation trials\n",
    "\n",
    "# --------------------------\n",
    "# Phoneme Vocab \n",
    "# --------------------------\n",
    "LOGIT_TO_PHONEME = [\n",
    "    \"BLANK\",  # index 0\n",
    "    \"AA\",\n",
    "    \"AE\",\n",
    "    \"AH\",\n",
    "    \"AO\",\n",
    "    \"AW\",\n",
    "    \"AY\",\n",
    "    \"B\",\n",
    "    \"CH\",\n",
    "    \"D\",\n",
    "    \"DH\",\n",
    "    \"EH\",\n",
    "    \"ER\",\n",
    "    \"EY\",\n",
    "    \"F\",\n",
    "    \"G\",\n",
    "    \"HH\",\n",
    "    \"IH\",\n",
    "    \"IY\",\n",
    "    \"JH\",\n",
    "    \"K\",\n",
    "    \"L\",\n",
    "    \"M\",\n",
    "    \"N\",\n",
    "    \"NG\",\n",
    "    \"OW\",\n",
    "    \"OY\",\n",
    "    \"P\",\n",
    "    \"R\",\n",
    "    \"S\",\n",
    "    \"SH\",\n",
    "    \"T\",\n",
    "    \"TH\",\n",
    "    \"UH\",\n",
    "    \"UW\",\n",
    "    \"V\",\n",
    "    \"W\",\n",
    "    \"Y\",\n",
    "    \"Z\",\n",
    "    \"ZH\",\n",
    "    \" | \",  # index 40 - silence/word boundary\n",
    "]\n",
    "\n",
    "VOCAB_SIZE = len(LOGIT_TO_PHONEME)  # 41 classes (including BLANK)\n",
    "phoneme_to_idx = {p: i for i, p in enumerate(LOGIT_TO_PHONEME)}\n",
    "idx_to_phoneme = {i: p for i, p in enumerate(LOGIT_TO_PHONEME)}\n",
    "\n",
    "print(f\"VOCAB_SIZE = {VOCAB_SIZE} (BLANK + 39 phonemes + silence)\")\n",
    "print(f\"Phoneme set: {LOGIT_TO_PHONEME[1:]}\")  # Skip BLANK for display\n",
    "\n",
    "# --------------------------\n",
    "# Full Dataset Loading Functions\n",
    "# --------------------------\n",
    "def get_all_hdf5_files(base_path, split_name):\n",
    "    \"\"\"Recursively find all HDF5 files for a given split\"\"\"\n",
    "    all_files = []\n",
    "    for date_folder in sorted(os.listdir(base_path)):\n",
    "        if date_folder.startswith(\"t15.\"):\n",
    "            fp = os.path.join(base_path, date_folder, f\"data_{split_name}.hdf5\")\n",
    "            if os.path.exists(fp):\n",
    "                all_files.append(fp)\n",
    "    return all_files\n",
    "\n",
    "def load_full_dataset_info(h5_paths):\n",
    "    \"\"\"\n",
    "    Load metadata from all HDF5 files to determine total dataset size\n",
    "    Returns: list of (file_path, key) tuples for all valid trials\n",
    "    \"\"\"\n",
    "    all_trials = []\n",
    "    total_trials = 0\n",
    "    \n",
    "    for h5_path in h5_paths:\n",
    "        if not os.path.exists(h5_path):\n",
    "            continue\n",
    "            \n",
    "        with h5py.File(h5_path, \"r\") as f:\n",
    "            file_trials = 0\n",
    "            for k in f.keys():\n",
    "                grp = f[k]\n",
    "                # Check if trial has seq_class_ids (phoneme labels)\n",
    "                if \"seq_class_ids\" in grp:\n",
    "                    all_trials.append((h5_path, k))\n",
    "                    file_trials += 1\n",
    "            \n",
    "            print(f\"  {os.path.basename(h5_path)}: {file_trials} trials\")\n",
    "            total_trials += file_trials\n",
    "    \n",
    "    print(f\"\\nTotal trials found: {total_trials}\")\n",
    "    return all_trials\n",
    "\n",
    "# --------------------------\n",
    "# Dataset (Full Data, No Downsampling)\n",
    "# --------------------------\n",
    "class Brain2TextDatasetFull(Dataset):\n",
    "    def __init__(self, trial_list, max_trials=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trial_list: list of (file_path, key) tuples\n",
    "            max_trials: optional limit on number of trials (None = use all)\n",
    "        \"\"\"\n",
    "        self.trials = trial_list if max_trials is None else trial_list[:max_trials]\n",
    "        self.file_cache = {}  # Cache open file handles\n",
    "        print(f\"Dataset initialized with {len(self.trials)} trials\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.trials)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path, key = self.trials[idx]\n",
    "        \n",
    "        # Open file (or use cached handle)\n",
    "        if file_path not in self.file_cache:\n",
    "            self.file_cache[file_path] = h5py.File(file_path, \"r\")\n",
    "        \n",
    "        f = self.file_cache[file_path]\n",
    "        grp = f[key]\n",
    "        \n",
    "        # Load neural features - NO downsampling, NO length cap\n",
    "        x = grp[\"input_features\"][()]  # (T, 512)\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        # Load phoneme target (seq_class_ids)\n",
    "        # These are ALREADY phoneme indices from the dataset\n",
    "        if \"seq_class_ids\" in grp:\n",
    "            seq_class_ids = grp[\"seq_class_ids\"][()]\n",
    "            seq_len = grp.attrs.get(\"seq_len\", len(seq_class_ids))\n",
    "            \n",
    "            # Extract valid phoneme sequence (up to seq_len, remove padding)\n",
    "            phoneme_seq = seq_class_ids[:seq_len]\n",
    "            \n",
    "            # Convert to tensor - these are already phoneme indices\n",
    "            y = torch.tensor(phoneme_seq, dtype=torch.long)\n",
    "        else:\n",
    "            # Fallback: empty sequence\n",
    "            y = torch.tensor([], dtype=torch.long)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def __del__(self):\n",
    "        # Close all cached file handles\n",
    "        for f in self.file_cache.values():\n",
    "            try:\n",
    "                f.close()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "def ctc_collate(batch):\n",
    "    \"\"\"Collate function for CTC loss\"\"\"\n",
    "    xs, ys = zip(*batch)\n",
    "    x_lens = torch.tensor([len(x) for x in xs], dtype=torch.long)\n",
    "    y_lens = torch.tensor([len(y) for y in ys], dtype=torch.long)\n",
    "    X = nn.utils.rnn.pad_sequence(xs, batch_first=True)\n",
    "    Y = torch.cat(ys)\n",
    "    return X, Y, x_lens, y_lens\n",
    "\n",
    "# --------------------------\n",
    "# Model\n",
    "# --------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class BrainTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(512, D_MODEL),\n",
    "            nn.LayerNorm(D_MODEL),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(DROPOUT)\n",
    "        )\n",
    "        self.pos = PositionalEncoding(D_MODEL)\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=D_MODEL, nhead=NHEAD, dim_feedforward=D_MODEL*4,\n",
    "            dropout=DROPOUT, activation='gelu', batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(layer, NUM_LAYERS)\n",
    "        self.out = nn.Linear(D_MODEL, VOCAB_SIZE)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.proj(x)\n",
    "        x = self.pos(x)\n",
    "        x = self.transformer(x, src_key_padding_mask=mask)\n",
    "        return self.out(x)\n",
    "\n",
    "# --------------------------\n",
    "# Phoneme Error Rate (PER) Calculation\n",
    "# --------------------------\n",
    "def ctc_greedy_decode(logits):\n",
    "    \"\"\"\n",
    "    CTC greedy decoding: argmax -> collapse repeats -> remove blanks\n",
    "    Matches the reference implementation\n",
    "    \n",
    "    Args:\n",
    "        logits: (batch, time, vocab_size) tensor\n",
    "    Returns:\n",
    "        List of phoneme index sequences (one per batch item)\n",
    "    \"\"\"\n",
    "    pred_ids = logits.argmax(-1).cpu().numpy()  # (batch, time)\n",
    "    \n",
    "    decoded_sequences = []\n",
    "    for seq in pred_ids:\n",
    "        # Collapse consecutive duplicates\n",
    "        collapsed = []\n",
    "        prev = -1\n",
    "        for token_id in seq:\n",
    "            if token_id != prev:\n",
    "                collapsed.append(int(token_id))\n",
    "            prev = token_id\n",
    "        \n",
    "        # Remove blanks (index 0)\n",
    "        no_blanks = [tok for tok in collapsed if tok != 0]\n",
    "        decoded_sequences.append(no_blanks)\n",
    "    \n",
    "    return decoded_sequences\n",
    "\n",
    "def calculate_per(predictions, ground_truths):\n",
    "    \"\"\"\n",
    "    Calculate Phoneme Error Rate using edit distance\n",
    "    Matches reference implementation\n",
    "    \n",
    "    Args:\n",
    "        predictions: list of predicted phoneme index sequences\n",
    "        ground_truths: list of ground truth phoneme index sequences\n",
    "    Returns:\n",
    "        per: Phoneme Error Rate as percentage\n",
    "        total_edit_distance: sum of edit distances\n",
    "        total_length: sum of ground truth lengths\n",
    "    \"\"\"\n",
    "    total_edit_distance = 0\n",
    "    total_length = 0\n",
    "    \n",
    "    for pred_seq, true_seq in zip(predictions, ground_truths):\n",
    "        # Calculate edit distance between sequences\n",
    "        edit_dist = editdistance.eval(true_seq, pred_seq)\n",
    "        total_edit_distance += edit_dist\n",
    "        total_length += len(true_seq)\n",
    "    \n",
    "    per = (total_edit_distance / total_length * 100) if total_length > 0 else 100.0\n",
    "    \n",
    "    return per, total_edit_distance, total_length\n",
    "\n",
    "# --------------------------\n",
    "# Validation (Full Dataset)\n",
    "# --------------------------\n",
    "def validate(model, dl):\n",
    "    \"\"\"Validate on full validation set\"\"\"\n",
    "    model.eval()\n",
    "    all_pred_seqs = []\n",
    "    all_true_seqs = []\n",
    "    \n",
    "    print(\"Starting validation...\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X, Y, x_len, y_len) in enumerate(dl):\n",
    "            X = X.to(DEVICE)\n",
    "            mask = torch.arange(X.size(1), device=DEVICE)[None, :] >= x_len.to(DEVICE)[:, None]\n",
    "            \n",
    "            logits = model(X, mask)  # (batch, time, vocab_size)\n",
    "            \n",
    "            # CTC greedy decode\n",
    "            pred_seqs = ctc_greedy_decode(logits)\n",
    "            \n",
    "            # Extract ground truth sequences\n",
    "            start = 0\n",
    "            true_seqs = []\n",
    "            for L in y_len:\n",
    "                seq = Y[start:start+L].cpu().numpy().tolist()\n",
    "                true_seqs.append(seq)\n",
    "                start += L\n",
    "            \n",
    "            all_pred_seqs.extend(pred_seqs)\n",
    "            all_true_seqs.extend(true_seqs)\n",
    "            \n",
    "            if (batch_idx + 1) % 50 == 0:\n",
    "                print(f\"  Validated {batch_idx + 1}/{len(dl)} batches\")\n",
    "            \n",
    "            del logits, X, mask\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Calculate PER\n",
    "    per, total_edit_dist, total_len = calculate_per(all_pred_seqs, all_true_seqs)\n",
    "    \n",
    "    print(f\"\\nValidation Results:\")\n",
    "    print(f\"  Total edit distance: {total_edit_dist}\")\n",
    "    print(f\"  Total sequence length: {total_len}\")\n",
    "    print(f\"  PER: {per:.2f}%\")\n",
    "    \n",
    "    # Get sample predictions for display (first 5)\n",
    "    sample_preds = []\n",
    "    sample_truths = []\n",
    "    for i in range(min(5, len(all_pred_seqs))):\n",
    "        pred_phonemes = [idx_to_phoneme[idx] for idx in all_pred_seqs[i] if idx < VOCAB_SIZE]\n",
    "        true_phonemes = [idx_to_phoneme[idx] for idx in all_true_seqs[i] if idx < VOCAB_SIZE]\n",
    "        sample_preds.append(' '.join(pred_phonemes))\n",
    "        sample_truths.append(' '.join(true_phonemes))\n",
    "    \n",
    "    return per, sample_preds, sample_truths\n",
    "\n",
    "# --------------------------\n",
    "# Main Training Loop\n",
    "# --------------------------\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING FULL DATASET...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get all HDF5 files\n",
    "train_files = get_all_hdf5_files(BASE_PATH, \"train\")\n",
    "val_files = get_all_hdf5_files(BASE_PATH, \"val\")\n",
    "\n",
    "print(f\"\\nFound {len(train_files)} training files\")\n",
    "print(f\"Found {len(val_files)} validation files\")\n",
    "\n",
    "# Load trial information\n",
    "print(\"\\nLoading training trials...\")\n",
    "train_trials = load_full_dataset_info(train_files)\n",
    "\n",
    "print(\"\\nLoading validation trials...\")\n",
    "val_trials = load_full_dataset_info(val_files)\n",
    "\n",
    "# Create datasets (full data)\n",
    "train_ds = Brain2TextDatasetFull(train_trials, max_trials=MAX_TRAIN_TRIALS)\n",
    "val_ds = Brain2TextDatasetFull(val_trials, max_trials=MAX_VAL_TRIALS)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dl = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    collate_fn=ctc_collate, num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE_VAL, shuffle=False,\n",
    "    collate_fn=ctc_collate, num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoader created:\")\n",
    "print(f\"  Train batches: {len(train_dl)}\")\n",
    "print(f\"  Val batches: {len(val_dl)}\")\n",
    "\n",
    "# --------------------------\n",
    "# Model + Optimizer\n",
    "# --------------------------\n",
    "model = BrainTransformer().to(DEVICE)\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=LEARNING_RATE, epochs=EPOCHS,\n",
    "    steps_per_epoch=len(train_dl), pct_start=0.1, anneal_strategy='cos'\n",
    ")\n",
    "ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "best_per = 100.0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING ON FULL DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (X, Y, x_len, y_len) in enumerate(train_dl):\n",
    "        X = X.to(DEVICE)\n",
    "        Y = Y.to(DEVICE)\n",
    "        x_len = x_len.to(DEVICE)\n",
    "        y_len = y_len.to(DEVICE)\n",
    "        \n",
    "        mask = torch.arange(X.size(1), device=DEVICE)[None,:] >= x_len[:,None]\n",
    "        \n",
    "        logits = model(X, mask)\n",
    "        log_probs = logits.log_softmax(-1).transpose(0, 1)  # (T, B, C)\n",
    "        \n",
    "        loss = ctc_loss(log_probs, Y, x_len, y_len)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"  Batch {batch_idx + 1}/{len(train_dl)} | Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dl)\n",
    "    print(f\"\\nEPOCH {epoch+1}/{EPOCHS} | Avg Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Validate every 3 epochs or in last 5 epochs\n",
    "    if (epoch+1) % 3 == 0 or epoch >= EPOCHS - 5:\n",
    "        print(\"\\nRunning full validation...\")\n",
    "        val_per, sample_preds, sample_truths = validate(model, val_dl)\n",
    "        \n",
    "        if val_per < best_per:\n",
    "            best_per = val_per\n",
    "            torch.save(model.state_dict(), SAVE_PATH)\n",
    "            print(\"✓ NEW BEST MODEL SAVED!\")\n",
    "        \n",
    "        print(\"\\nSample predictions (phonemes):\")\n",
    "        for i, (truth, pred) in enumerate(zip(sample_truths[:3], sample_preds[:3])):\n",
    "            print(f\"Sample {i+1}:\")\n",
    "            print(f\"  Truth: {truth}\")\n",
    "            print(f\"  Pred:  {pred}\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training complete!\")\n",
    "print(f\"Best PER: {best_per:.2f}%\")\n",
    "print(f\"Model saved: {SAVE_PATH}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc98ba95",
   "metadata": {
    "papermill": {
     "duration": 0.035368,
     "end_time": "2025-11-27T20:32:45.620914",
     "exception": false,
     "start_time": "2025-11-27T20:32:45.585546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13056355,
     "sourceId": 106809,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7554.763673,
   "end_time": "2025-11-27T20:32:48.180375",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-27T18:26:53.416702",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
