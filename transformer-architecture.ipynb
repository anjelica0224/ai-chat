{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":106809,"databundleVersionId":13056355,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================================\n# Brain-to-Text CTC Training - Full Dataset + PER\n# Using correct phoneme vocabulary from reference\n# ================================================\nimport os\nimport h5py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nimport editdistance\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# --------------------------\n# CONFIG\n# --------------------------\nBASE_PATH = \"/kaggle/input/brain-to-text-25/t15_copyTask_neuralData/hdf5_data_final\"\nBATCH_SIZE = 4  # Reduced for full dataset to avoid OOM\nBATCH_SIZE_VAL = 2\nNUM_WORKERS = 2\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nEPOCHS = 30\nLEARNING_RATE = 5e-4\nWEIGHT_DECAY = 0.01\nSAVE_PATH = \"best_model_full_per.pt\"\n\n# Model architecture\nD_MODEL = 256\nNHEAD = 8\nNUM_LAYERS = 4\nDROPOUT = 0.1\n\n# NO downsampling, NO max sequence length limit\nDOWNSAMPLE_FACTOR = 1  # No downsampling\nMAX_SEQ_LEN = None  # No length cap\n\n# Use ALL data - no subset limits\nMAX_TRAIN_TRIALS = None  # Use all training trials\nMAX_VAL_TRIALS = None  # Use all validation trials\n\n# --------------------------\n# Phoneme Vocab (EXACT match to reference code)\n# --------------------------\n# From the reference evaluation script\nLOGIT_TO_PHONEME = [\n    \"BLANK\",  # index 0\n    \"AA\",\n    \"AE\",\n    \"AH\",\n    \"AO\",\n    \"AW\",\n    \"AY\",\n    \"B\",\n    \"CH\",\n    \"D\",\n    \"DH\",\n    \"EH\",\n    \"ER\",\n    \"EY\",\n    \"F\",\n    \"G\",\n    \"HH\",\n    \"IH\",\n    \"IY\",\n    \"JH\",\n    \"K\",\n    \"L\",\n    \"M\",\n    \"N\",\n    \"NG\",\n    \"OW\",\n    \"OY\",\n    \"P\",\n    \"R\",\n    \"S\",\n    \"SH\",\n    \"T\",\n    \"TH\",\n    \"UH\",\n    \"UW\",\n    \"V\",\n    \"W\",\n    \"Y\",\n    \"Z\",\n    \"ZH\",\n    \" | \",  # index 40 - silence/word boundary\n]\n\nVOCAB_SIZE = len(LOGIT_TO_PHONEME)  # 41 classes (including BLANK)\nphoneme_to_idx = {p: i for i, p in enumerate(LOGIT_TO_PHONEME)}\nidx_to_phoneme = {i: p for i, p in enumerate(LOGIT_TO_PHONEME)}\n\nprint(f\"VOCAB_SIZE = {VOCAB_SIZE} (BLANK + 39 phonemes + silence)\")\nprint(f\"Phoneme set: {LOGIT_TO_PHONEME[1:]}\")  # Skip BLANK for display\n\n# --------------------------\n# Full Dataset Loading Functions\n# --------------------------\ndef get_all_hdf5_files(base_path, split_name):\n    \"\"\"Recursively find all HDF5 files for a given split\"\"\"\n    all_files = []\n    for date_folder in sorted(os.listdir(base_path)):\n        if date_folder.startswith(\"t15.\"):\n            fp = os.path.join(base_path, date_folder, f\"data_{split_name}.hdf5\")\n            if os.path.exists(fp):\n                all_files.append(fp)\n    return all_files\n\ndef load_full_dataset_info(h5_paths):\n    \"\"\"\n    Load metadata from all HDF5 files to determine total dataset size\n    Returns: list of (file_path, key) tuples for all valid trials\n    \"\"\"\n    all_trials = []\n    total_trials = 0\n    \n    for h5_path in h5_paths:\n        if not os.path.exists(h5_path):\n            continue\n            \n        with h5py.File(h5_path, \"r\") as f:\n            file_trials = 0\n            for k in f.keys():\n                grp = f[k]\n                # Check if trial has seq_class_ids (phoneme labels)\n                if \"seq_class_ids\" in grp:\n                    all_trials.append((h5_path, k))\n                    file_trials += 1\n            \n            print(f\"  {os.path.basename(h5_path)}: {file_trials} trials\")\n            total_trials += file_trials\n    \n    print(f\"\\nTotal trials found: {total_trials}\")\n    return all_trials\n\n# --------------------------\n# Dataset (Full Data, No Downsampling)\n# --------------------------\nclass Brain2TextDatasetFull(Dataset):\n    def __init__(self, trial_list, max_trials=None):\n        \"\"\"\n        Args:\n            trial_list: list of (file_path, key) tuples\n            max_trials: optional limit on number of trials (None = use all)\n        \"\"\"\n        self.trials = trial_list if max_trials is None else trial_list[:max_trials]\n        self.file_cache = {}  # Cache open file handles\n        print(f\"Dataset initialized with {len(self.trials)} trials\")\n    \n    def __len__(self):\n        return len(self.trials)\n    \n    def __getitem__(self, idx):\n        file_path, key = self.trials[idx]\n        \n        # Open file (or use cached handle)\n        if file_path not in self.file_cache:\n            self.file_cache[file_path] = h5py.File(file_path, \"r\")\n        \n        f = self.file_cache[file_path]\n        grp = f[key]\n        \n        # Load neural features - NO downsampling, NO length cap\n        x = grp[\"input_features\"][()]  # (T, 512)\n        x = torch.tensor(x, dtype=torch.float32)\n        \n        # Load phoneme target (seq_class_ids)\n        # These are ALREADY phoneme indices from the dataset\n        if \"seq_class_ids\" in grp:\n            seq_class_ids = grp[\"seq_class_ids\"][()]\n            seq_len = grp.attrs.get(\"seq_len\", len(seq_class_ids))\n            \n            # Extract valid phoneme sequence (up to seq_len, remove padding)\n            phoneme_seq = seq_class_ids[:seq_len]\n            \n            # Convert to tensor - these are already phoneme indices\n            y = torch.tensor(phoneme_seq, dtype=torch.long)\n        else:\n            # Fallback: empty sequence\n            y = torch.tensor([], dtype=torch.long)\n        \n        return x, y\n    \n    def __del__(self):\n        # Close all cached file handles\n        for f in self.file_cache.values():\n            try:\n                f.close()\n            except:\n                pass\n\ndef ctc_collate(batch):\n    \"\"\"Collate function for CTC loss\"\"\"\n    xs, ys = zip(*batch)\n    x_lens = torch.tensor([len(x) for x in xs], dtype=torch.long)\n    y_lens = torch.tensor([len(y) for y in ys], dtype=torch.long)\n    X = nn.utils.rnn.pad_sequence(xs, batch_first=True)\n    Y = torch.cat(ys)\n    return X, Y, x_lens, y_lens\n\n# --------------------------\n# Model\n# --------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=10000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer('pe', pe.unsqueeze(0))\n    \n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\nclass BrainTransformer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.proj = nn.Sequential(\n            nn.Linear(512, D_MODEL),\n            nn.LayerNorm(D_MODEL),\n            nn.GELU(),\n            nn.Dropout(DROPOUT)\n        )\n        self.pos = PositionalEncoding(D_MODEL)\n        layer = nn.TransformerEncoderLayer(\n            d_model=D_MODEL, nhead=NHEAD, dim_feedforward=D_MODEL*4,\n            dropout=DROPOUT, activation='gelu', batch_first=True, norm_first=True\n        )\n        self.transformer = nn.TransformerEncoder(layer, NUM_LAYERS)\n        self.out = nn.Linear(D_MODEL, VOCAB_SIZE)\n    \n    def forward(self, x, mask=None):\n        x = self.proj(x)\n        x = self.pos(x)\n        x = self.transformer(x, src_key_padding_mask=mask)\n        return self.out(x)\n\n# --------------------------\n# Phoneme Error Rate (PER) Calculation\n# Matches reference implementation exactly\n# --------------------------\ndef ctc_greedy_decode(logits):\n    \"\"\"\n    CTC greedy decoding: argmax -> collapse repeats -> remove blanks\n    Matches the reference implementation\n    \n    Args:\n        logits: (batch, time, vocab_size) tensor\n    Returns:\n        List of phoneme index sequences (one per batch item)\n    \"\"\"\n    pred_ids = logits.argmax(-1).cpu().numpy()  # (batch, time)\n    \n    decoded_sequences = []\n    for seq in pred_ids:\n        # Collapse consecutive duplicates\n        collapsed = []\n        prev = -1\n        for token_id in seq:\n            if token_id != prev:\n                collapsed.append(int(token_id))\n            prev = token_id\n        \n        # Remove blanks (index 0)\n        no_blanks = [tok for tok in collapsed if tok != 0]\n        decoded_sequences.append(no_blanks)\n    \n    return decoded_sequences\n\ndef calculate_per(predictions, ground_truths):\n    \"\"\"\n    Calculate Phoneme Error Rate using edit distance\n    Matches reference implementation\n    \n    Args:\n        predictions: list of predicted phoneme index sequences\n        ground_truths: list of ground truth phoneme index sequences\n    Returns:\n        per: Phoneme Error Rate as percentage\n        total_edit_distance: sum of edit distances\n        total_length: sum of ground truth lengths\n    \"\"\"\n    total_edit_distance = 0\n    total_length = 0\n    \n    for pred_seq, true_seq in zip(predictions, ground_truths):\n        # Calculate edit distance between sequences\n        edit_dist = editdistance.eval(true_seq, pred_seq)\n        total_edit_distance += edit_dist\n        total_length += len(true_seq)\n    \n    per = (total_edit_distance / total_length * 100) if total_length > 0 else 100.0\n    \n    return per, total_edit_distance, total_length\n\n# --------------------------\n# Validation (Full Dataset)\n# --------------------------\ndef validate(model, dl):\n    \"\"\"Validate on full validation set\"\"\"\n    model.eval()\n    all_pred_seqs = []\n    all_true_seqs = []\n    \n    print(\"Starting validation...\")\n    with torch.no_grad():\n        for batch_idx, (X, Y, x_len, y_len) in enumerate(dl):\n            X = X.to(DEVICE)\n            mask = torch.arange(X.size(1), device=DEVICE)[None, :] >= x_len.to(DEVICE)[:, None]\n            \n            logits = model(X, mask)  # (batch, time, vocab_size)\n            \n            # CTC greedy decode\n            pred_seqs = ctc_greedy_decode(logits)\n            \n            # Extract ground truth sequences\n            start = 0\n            true_seqs = []\n            for L in y_len:\n                seq = Y[start:start+L].cpu().numpy().tolist()\n                true_seqs.append(seq)\n                start += L\n            \n            all_pred_seqs.extend(pred_seqs)\n            all_true_seqs.extend(true_seqs)\n            \n            if (batch_idx + 1) % 50 == 0:\n                print(f\"  Validated {batch_idx + 1}/{len(dl)} batches\")\n            \n            del logits, X, mask\n            torch.cuda.empty_cache()\n    \n    # Calculate PER\n    per, total_edit_dist, total_len = calculate_per(all_pred_seqs, all_true_seqs)\n    \n    print(f\"\\nValidation Results:\")\n    print(f\"  Total edit distance: {total_edit_dist}\")\n    print(f\"  Total sequence length: {total_len}\")\n    print(f\"  PER: {per:.2f}%\")\n    \n    # Get sample predictions for display (first 5)\n    sample_preds = []\n    sample_truths = []\n    for i in range(min(5, len(all_pred_seqs))):\n        pred_phonemes = [idx_to_phoneme[idx] for idx in all_pred_seqs[i] if idx < VOCAB_SIZE]\n        true_phonemes = [idx_to_phoneme[idx] for idx in all_true_seqs[i] if idx < VOCAB_SIZE]\n        sample_preds.append(' '.join(pred_phonemes))\n        sample_truths.append(' '.join(true_phonemes))\n    \n    return per, sample_preds, sample_truths\n\n# --------------------------\n# Main Training Loop\n# --------------------------\nprint(\"=\"*60)\nprint(\"LOADING FULL DATASET...\")\nprint(\"=\"*60)\n\n# Get all HDF5 files\ntrain_files = get_all_hdf5_files(BASE_PATH, \"train\")\nval_files = get_all_hdf5_files(BASE_PATH, \"val\")\n\nprint(f\"\\nFound {len(train_files)} training files\")\nprint(f\"Found {len(val_files)} validation files\")\n\n# Load trial information\nprint(\"\\nLoading training trials...\")\ntrain_trials = load_full_dataset_info(train_files)\n\nprint(\"\\nLoading validation trials...\")\nval_trials = load_full_dataset_info(val_files)\n\n# Create datasets (full data)\ntrain_ds = Brain2TextDatasetFull(train_trials, max_trials=MAX_TRAIN_TRIALS)\nval_ds = Brain2TextDatasetFull(val_trials, max_trials=MAX_VAL_TRIALS)\n\n# Create dataloaders\ntrain_dl = DataLoader(\n    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n    collate_fn=ctc_collate, num_workers=NUM_WORKERS, pin_memory=True\n)\nval_dl = DataLoader(\n    val_ds, batch_size=BATCH_SIZE_VAL, shuffle=False,\n    collate_fn=ctc_collate, num_workers=NUM_WORKERS, pin_memory=True\n)\n\nprint(f\"\\nDataLoader created:\")\nprint(f\"  Train batches: {len(train_dl)}\")\nprint(f\"  Val batches: {len(val_dl)}\")\n\n# --------------------------\n# Model + Optimizer\n# --------------------------\nmodel = BrainTransformer().to(DEVICE)\nprint(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer, max_lr=LEARNING_RATE, epochs=EPOCHS,\n    steps_per_epoch=len(train_dl), pct_start=0.1, anneal_strategy='cos'\n)\nctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n\nbest_per = 100.0\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING TRAINING ON FULL DATASET\")\nprint(\"=\"*60)\n\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n    \n    for batch_idx, (X, Y, x_len, y_len) in enumerate(train_dl):\n        X = X.to(DEVICE)\n        Y = Y.to(DEVICE)\n        x_len = x_len.to(DEVICE)\n        y_len = y_len.to(DEVICE)\n        \n        mask = torch.arange(X.size(1), device=DEVICE)[None,:] >= x_len[:,None]\n        \n        logits = model(X, mask)\n        log_probs = logits.log_softmax(-1).transpose(0, 1)  # (T, B, C)\n        \n        loss = ctc_loss(log_probs, Y, x_len, y_len)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        \n        total_loss += loss.item()\n        \n        if (batch_idx + 1) % 100 == 0:\n            print(f\"  Batch {batch_idx + 1}/{len(train_dl)} | Loss: {loss.item():.4f}\")\n    \n    avg_loss = total_loss / len(train_dl)\n    print(f\"\\nEPOCH {epoch+1}/{EPOCHS} | Avg Loss: {avg_loss:.4f}\")\n    \n    # Validate every 3 epochs or in last 5 epochs\n    if (epoch+1) % 3 == 0 or epoch >= EPOCHS - 5:\n        print(\"\\nRunning full validation...\")\n        val_per, sample_preds, sample_truths = validate(model, val_dl)\n        \n        if val_per < best_per:\n            best_per = val_per\n            torch.save(model.state_dict(), SAVE_PATH)\n            print(\"âœ“ NEW BEST MODEL SAVED!\")\n        \n        print(\"\\nSample predictions (phonemes):\")\n        for i, (truth, pred) in enumerate(zip(sample_truths[:3], sample_preds[:3])):\n            print(f\"Sample {i+1}:\")\n            print(f\"  Truth: {truth}\")\n            print(f\"  Pred:  {pred}\")\n        print(\"-\"*60)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Training complete!\")\nprint(f\"Best PER: {best_per:.2f}%\")\nprint(f\"Model saved: {SAVE_PATH}\")\nprint(f\"{'='*60}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T15:54:28.524975Z","iopub.execute_input":"2025-11-27T15:54:28.525189Z","iopub.status.idle":"2025-11-27T17:59:35.759773Z","shell.execute_reply.started":"2025-11-27T15:54:28.525157Z","shell.execute_reply":"2025-11-27T17:59:35.758613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}